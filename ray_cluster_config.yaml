# Ray Cluster Configuration for CNN+LSTM Training
# Optimized for distributed hyperparameter optimization and training

cluster_name: cnn-lstm-training

# Resource allocation
max_workers: 8
upscaling_speed: 1.0
idle_timeout_minutes: 5

# Docker configuration for reproducible environments
docker:
  image: "rayproject/ray-ml:latest-gpu"
  container_name: "ray_cnn_lstm"
  pull_before_run: True
  run_options:
    - "--gpus=all"
    - "--shm-size=2g"

# Head node configuration
head_node:
  resources:
    CPU: 8
    GPU: 1
    memory: 16000000000 # 16GB
    object_store_memory: 4000000000 # 4GB
  node_config:
    InstanceType: g4dn.2xlarge # AWS GPU instance
    ImageId: ami-0123456789abcdef0 # Replace with appropriate AMI
    SecurityGroupIds: ["sg-12345678"]
    SubnetId: subnet-12345678
    IamInstanceProfile:
      Arn: "arn:aws:iam::123456789012:instance-profile/ray-autoscaler-v1"

# Worker node configuration
worker_nodes:
  default_worker:
    min_workers: 0
    max_workers: 4
    resources:
      CPU: 4
      GPU: 1
      memory: 8000000000 # 8GB
      object_store_memory: 2000000000 # 2GB
    node_config:
      InstanceType: g4dn.xlarge
      ImageId: ami-0123456789abcdef0
      SecurityGroupIds: ["sg-12345678"]
      SubnetId: subnet-12345678
      IamInstanceProfile:
        Arn: "arn:aws:iam::123456789012:instance-profile/ray-autoscaler-v1"

# Setup commands for all nodes
setup_commands:
  # Update system
  - sudo apt-get update -y
  - sudo apt-get install -y git wget curl

  # Install Python dependencies
  - pip install --upgrade pip
  - pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
  - pip install ray[tune,train] optuna sklearn pandas numpy matplotlib seaborn tqdm
  - pip install yfinance pandas-ta

  # Clone and setup project
  - git clone https://github.com/your-repo/trading-rl-agent.git /home/ray/trading-rl-agent || true
  - cd /home/ray/trading-rl-agent && pip install -e .

# Head node specific setup
head_setup_commands:
  - echo "Setting up head node for CNN+LSTM training cluster"
  - mkdir -p /home/ray/shared_storage
  - chmod 777 /home/ray/shared_storage

# Worker node specific setup
worker_setup_commands:
  - echo "Setting up worker node for CNN+LSTM training"
  - mkdir -p /home/ray/shared_storage
  - chmod 777 /home/ray/shared_storage

# File mounts for sharing data across nodes
file_mounts:
  "/home/ray/trading-rl-agent": "/home/ray/trading-rl-agent"
  "/home/ray/shared_storage": "/home/ray/shared_storage"

# Environment variables
environment_variables:
  RAY_DISABLE_IMPORT_WARNING: "1"
  CUDA_VISIBLE_DEVICES: "0"
  PYTHONPATH: "/home/ray/trading-rl-agent/src:$PYTHONPATH"

# Cluster initialization commands
head_start_ray_commands:
  - ray stop
  - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0 --dashboard-port=8265

worker_start_ray_commands:
  - ray stop
  - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076

# Authentication and security
auth:
  ssh_user: ray
  ssh_private_key: ~/.ssh/ray-autoscaler_us-west-2.pem

# Provider-specific configuration (AWS example)
provider:
  type: aws
  region: us-west-2
  availability_zone: us-west-2a
  cache_stopped_nodes: False

  # Security configuration
  security_group:
    GroupName: ray-autoscaler-cnn-lstm
    Rules:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 6379
        ToPort: 6379
        SourceSecurityGroupName: ray-autoscaler-cnn-lstm
      - IpProtocol: tcp
        FromPort: 8265
        ToPort: 8265
        CidrIp: 0.0.0.0/0 # Dashboard access
