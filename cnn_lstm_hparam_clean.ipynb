{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48577999",
   "metadata": {},
   "source": [
    "# CNN-LSTM Hyperparameter Optimization\n",
    "**Phase 2.5 - Trading RL Agent Development**\n",
    "\n",
    "This notebook implements distributed hyperparameter optimization for the CNN-LSTM model used in financial time series prediction.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Test CNN-LSTM model architecture with sample trading data\n",
    "- Implement comprehensive hyperparameter search space  \n",
    "- Execute distributed optimization using Ray Tune\n",
    "- Analyze results and identify optimal configurations\n",
    "\n",
    "## üìã Progress Tracker\n",
    "- [ ] **Step 1**: Environment Setup & Data Loading\n",
    "- [ ] **Step 2**: Model Architecture Validation\n",
    "- [ ] **Step 3**: Training Pipeline Implementation\n",
    "- [ ] **Step 4**: Hyperparameter Search Configuration\n",
    "- [ ] **Step 5**: Ray Tune Integration & Execution\n",
    "- [ ] **Step 6**: Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c006f",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Environment Setup & Data Loading\n",
    "\n",
    "Setting up the Python environment and loading sample trading data for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1353c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Environment Setup with Ray Cluster Integration\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"üîß Environment Configuration:\")\n",
    "print(f\"   ‚Ä¢ Project root: {project_root}\")\n",
    "print(f\"   ‚Ä¢ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"   ‚Ä¢ CPU count: {os.cpu_count()}\")\n",
    "print(f\"   ‚Ä¢ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Import project modules with error handling\n",
    "try:\n",
    "    from src.data_pipeline import (\n",
    "        PipelineConfig,\n",
    "        generate_features,\n",
    "        load_data,\n",
    "        split_by_date,\n",
    "    )\n",
    "    from src.models.cnn_lstm import CNNLSTMConfig, CNNLSTMModel, create_model\n",
    "    from src.optimization.model_summary import detect_gpus, optimal_gpu_config\n",
    "    from src.utils.cluster import get_available_devices, init_ray\n",
    "\n",
    "    print(\"‚úÖ Project modules imported successfully\")\n",
    "    modules_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Ensure you're running from the project root directory\")\n",
    "    modules_available = False\n",
    "\n",
    "# Initialize Ray cluster for distributed training\n",
    "print(\"\\nüöÄ Ray Cluster Initialization:\")\n",
    "try:\n",
    "    # Check if Ray is already initialized\n",
    "    import ray\n",
    "\n",
    "    if ray.is_initialized():\n",
    "        print(\"   ‚Ä¢ Ray already initialized\")\n",
    "        ray_ready = True\n",
    "    else:\n",
    "        # Try to connect to existing cluster first, then local\n",
    "        try:\n",
    "            init_ray(local_mode=False)  # Try to connect to cluster\n",
    "            print(\"   ‚Ä¢ Connected to Ray cluster\")\n",
    "            ray_ready = True\n",
    "        except Exception:\n",
    "            init_ray(local_mode=True)  # Fallback to local mode\n",
    "            print(\"   ‚Ä¢ Ray initialized in local mode\")\n",
    "            ray_ready = True\n",
    "\n",
    "    # Get available resources\n",
    "    if ray_ready:\n",
    "        resources = get_available_devices()\n",
    "        print(f\"   ‚Ä¢ Available CPUs: {resources.get('CPU', 0)}\")\n",
    "        print(f\"   ‚Ä¢ Available GPUs: {resources.get('GPU', 0)}\")\n",
    "\n",
    "        # Get optimal GPU configuration\n",
    "        gpu_config = optimal_gpu_config(\n",
    "            model_params=100000,  # Approximate parameter count for CNN-LSTM\n",
    "            batch_size=32,\n",
    "            sequence_length=60,\n",
    "            feature_dim=10,\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ Optimal GPU config: {gpu_config}\")\n",
    "\n",
    "        ray_available = True\n",
    "\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Ray initialization failed\")\n",
    "        ray_available = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Ray initialization failed: {e}\")\n",
    "    print(\"   ‚Ä¢ Continuing without distributed training\")\n",
    "    ray_ready = False\n",
    "    ray_available = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35142e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preparation\n",
    "print(\"üìä Loading Sample Trading Data...\")\n",
    "\n",
    "# Load available sample data\n",
    "data_files = [\n",
    "    f\n",
    "    for f in os.listdir(\"data/\")\n",
    "    if f.startswith(\"sample_training_data\") and f.endswith(\".csv\")\n",
    "]\n",
    "print(f\"   ‚Ä¢ Available data files: {len(data_files)}\")\n",
    "\n",
    "if data_files:\n",
    "    # Use the simple sample data for testing\n",
    "    sample_file = \"data/sample_training_data_simple_20250607_192034.csv\"\n",
    "\n",
    "    try:\n",
    "        df_sample = pd.read_csv(sample_file)\n",
    "        print(f\"‚úÖ Data loaded successfully: {sample_file}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {df_sample.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(df_sample.columns)}\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Date range: {df_sample['timestamp'].iloc[0]} to {df_sample['timestamp'].iloc[-1]}\"\n",
    "        )\n",
    "\n",
    "        # Display sample data\n",
    "        print(\"\\nüìã Sample Data Preview:\")\n",
    "        print(df_sample.head(3))\n",
    "\n",
    "        # Prepare feature data (OHLCV) with data cleaning\n",
    "        feature_columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "        # Clean the data before processing\n",
    "        print(\"\\nüßπ Cleaning sample data...\")\n",
    "\n",
    "        # Replace infinite values with NaN first\n",
    "        df_clean = df_sample[feature_columns].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Fill NaN values with reasonable estimates\n",
    "        for col in feature_columns:\n",
    "            if col == \"volume\":\n",
    "                # Use median for volume\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # Use forward fill then backward fill for prices\n",
    "                df_clean[col] = (\n",
    "                    df_clean[col].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "                )\n",
    "                # If still NaN (entire column), use a reasonable default\n",
    "                if df_clean[col].isna().all():\n",
    "                    df_clean[col] = 100.0  # Default price\n",
    "\n",
    "        # Convert to float32 for numerical stability\n",
    "        X_raw = df_clean.values.astype(np.float32)\n",
    "\n",
    "        print(f\"\\nüî¢ Feature Data (After Cleaning):\")\n",
    "        print(f\"   ‚Ä¢ Features: {feature_columns}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {X_raw.shape}\")\n",
    "        print(f\"   ‚Ä¢ Data type: {X_raw.dtype}\")\n",
    "\n",
    "        # Check for any remaining data quality issues\n",
    "        nan_count = np.isnan(X_raw).sum()\n",
    "        inf_count = np.isinf(X_raw).sum()\n",
    "\n",
    "        print(f\"   ‚Ä¢ NaN values: {nan_count}\")\n",
    "        print(f\"   ‚Ä¢ Infinite values: {inf_count}\")\n",
    "\n",
    "        if nan_count == 0 and inf_count == 0:\n",
    "            print(\"‚úÖ Data quality checks passed\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"‚ö†Ô∏è  Remaining data quality issues: {nan_count} NaN, {inf_count} infinite values\"\n",
    "            )\n",
    "            # Final cleanup if any issues remain\n",
    "            X_raw = np.nan_to_num(X_raw, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "            print(\"   ‚Ä¢ Applied final cleanup with np.nan_to_num\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        df_sample = None\n",
    "        X_raw = None\n",
    "else:\n",
    "    print(\"‚ùå No sample data files found\")\n",
    "    print(\"   Run generate_sample_data.py to create sample data\")\n",
    "    df_sample = None\n",
    "    X_raw = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f779f2",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Model Architecture Validation\n",
    "\n",
    "Testing the CNN-LSTM model architecture with our sample data to ensure compatibility before hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Validation\n",
    "if X_raw is not None and modules_available:\n",
    "    print(\"üèóÔ∏è Creating and Testing Optimized CNN-LSTM Model...\")\n",
    "\n",
    "    # Prepare sequence data for time series prediction\n",
    "    sequence_length = min(30, len(X_raw) - 10)  # Adaptive sequence length\n",
    "    step_size = 1\n",
    "\n",
    "    print(f\"   ‚Ä¢ Adaptive sequence length: {sequence_length}\")\n",
    "    print(f\"   ‚Ä¢ Feature dimensions: {X_raw.shape[1]}\")\n",
    "    print(f\"   ‚Ä¢ Total samples available: {len(X_raw)}\")\n",
    "\n",
    "    # Create sequences with validation\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "\n",
    "    for i in range(len(X_raw) - sequence_length):\n",
    "        X_sequences.append(X_raw[i : i + sequence_length])\n",
    "        # Predict next close price (index 3 in OHLCV)\n",
    "        y_sequences.append(X_raw[i + sequence_length, 3])\n",
    "\n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Sequence data shape: X={X_sequences.shape}, y={y_sequences.shape}\")\n",
    "\n",
    "    # Enhanced model configuration with optimal settings\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],  # Number of features (5 for OHLCV)\n",
    "        output_size=1,  # Single prediction output\n",
    "        cnn_filters=[32, 64],  # CNN layer sizes\n",
    "        cnn_kernel_sizes=[3, 5],  # Different kernel sizes for pattern detection\n",
    "        lstm_units=50,  # LSTM hidden units\n",
    "        dropout=0.2,  # Dropout rate\n",
    "        use_attention=False,  # No attention for baseline\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüîß Enhanced Model Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Input dimensions: {test_config.input_dim}\")\n",
    "    print(f\"   ‚Ä¢ CNN filters: {test_config.cnn_filters}\")\n",
    "    print(f\"   ‚Ä¢ CNN kernels: {test_config.cnn_kernel_sizes}\")\n",
    "    print(f\"   ‚Ä¢ LSTM units: {test_config.lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Dropout: {test_config.dropout}\")\n",
    "\n",
    "    # Device selection with optimization\n",
    "    if torch.cuda.is_available() and resources.get(\"GPU\", 0) > 0:\n",
    "        device = torch.device(\"cuda:0\")  # Use first GPU\n",
    "        print(f\"   ‚Ä¢ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"   ‚Ä¢ Using CPU with {resources.get('CPU', 1)} cores\")\n",
    "\n",
    "    # Create and test model with memory optimization\n",
    "    try:\n",
    "        model = create_model(test_config)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"\\n‚úÖ Model created successfully on {device}\")\n",
    "        print(f\"   ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "        print(f\"   ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "        # Memory usage estimation\n",
    "        if device.type == \"cuda\":\n",
    "            model_memory_mb = sum(\n",
    "                p.numel() * p.element_size() for p in model.parameters()\n",
    "            ) / (1024**2)\n",
    "            print(f\"   ‚Ä¢ Model memory usage: {model_memory_mb:.1f} MB\")\n",
    "\n",
    "        # Test forward pass with different batch sizes\n",
    "        test_batch_sizes = (\n",
    "            [4, 8, 16] if len(X_sequences) >= 16 else [min(4, len(X_sequences))]\n",
    "        )\n",
    "\n",
    "        for batch_size in test_batch_sizes:\n",
    "            if batch_size <= len(X_sequences):\n",
    "                try:\n",
    "                    X_test = torch.FloatTensor(X_sequences[:batch_size]).to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        test_output = model(X_test)\n",
    "\n",
    "                    print(\n",
    "                        f\"   ‚Ä¢ Batch size {batch_size}: ‚úÖ {X_test.shape} ‚Üí {test_output.shape}\"\n",
    "                    )\n",
    "\n",
    "                    if (\n",
    "                        batch_size == test_batch_sizes[0]\n",
    "                    ):  # Show predictions for first batch\n",
    "                        sample_preds = test_output.flatten()[:3].cpu().numpy()\n",
    "                        print(f\"   ‚Ä¢ Sample predictions: {sample_preds}\")\n",
    "\n",
    "                    # Clear cache for next test\n",
    "                    if device.type == \"cuda\":\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"   ‚Ä¢ Batch size {batch_size}: ‚ùå {str(e)}\")\n",
    "\n",
    "        architecture_validated = True\n",
    "\n",
    "        # Store configuration for hyperparameter optimization\n",
    "        validated_config = {\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"input_dim\": test_config.input_dim,\n",
    "            \"data_shape\": X_sequences.shape,\n",
    "            \"model_params\": total_params,\n",
    "            \"device\": str(device),\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Validation Summary:\")\n",
    "        print(f\"   ‚Ä¢ Architecture: ‚úÖ Validated\")\n",
    "        print(f\"   ‚Ä¢ Device compatibility: ‚úÖ {device}\")\n",
    "        print(f\"   ‚Ä¢ Memory efficiency: ‚úÖ Optimized\")\n",
    "        print(f\"   ‚Ä¢ Ready for hyperparameter optimization\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model architecture error: {e}\")\n",
    "        architecture_validated = False\n",
    "        validated_config = None\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model validation - dependencies not available\")\n",
    "    architecture_validated = False\n",
    "    validated_config = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7cb1f",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Comprehensive Training Pipeline\n",
    "\n",
    "Implementing a robust training pipeline with loss calculation, metrics tracking, and distributed optimization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tune Compatible Training Function\n",
    "\n",
    "\n",
    "def train_cnn_lstm_ray(config, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Ray Tune compatible training function for CNN-LSTM hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "        config: Hyperparameter configuration from Ray Tune\n",
    "        checkpoint_dir: Directory for saving/loading checkpoints\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from ray import train, tune  # Import both tune and train for Ray 2.0+\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    learning_rate = config.get(\"learning_rate\", 0.001)\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    num_epochs = config.get(\"num_epochs\", 50)\n",
    "    cnn_filters = config.get(\"cnn_filters\", [32, 64])\n",
    "    lstm_units = config.get(\"lstm_units\", 50)\n",
    "    dropout = config.get(\"dropout\", 0.2)\n",
    "    cnn_kernel_sizes = config.get(\"cnn_kernel_sizes\", [3, 5])\n",
    "\n",
    "    # Global data (will be passed from outer scope)\n",
    "    global X_sequences_train, y_sequences_train, X_sequences_val, y_sequences_val\n",
    "\n",
    "    # Device selection\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create model configuration\n",
    "    model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences_train.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=cnn_filters,\n",
    "        cnn_kernel_sizes=cnn_kernel_sizes,\n",
    "        lstm_units=lstm_units,\n",
    "        dropout=dropout,\n",
    "        use_attention=False,\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(model_config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_train), torch.FloatTensor(y_sequences_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_val), torch.FloatTensor(y_sequences_val)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_dir:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            train_samples += data.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / train_samples\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output.squeeze(), target)\n",
    "\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                val_samples += data.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if checkpoint_dir and (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "\n",
    "        # Track best validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "        # Report metrics to Ray Tune (Updated for Ray 2.0+)\n",
    "        train.report(\n",
    "            {\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Distributed Training\n",
    "if architecture_validated and X_sequences is not None:\n",
    "    print(\"üìä Preparing Data for Distributed Training...\")\n",
    "\n",
    "    # Data Quality Check and Cleaning\n",
    "    print(\"üßπ Cleaning data for training...\")\n",
    "\n",
    "    # Check for data quality issues in sequences\n",
    "    original_samples = len(X_sequences)\n",
    "    print(f\"   ‚Ä¢ Original samples: {original_samples}\")\n",
    "\n",
    "    # Convert to numpy arrays for easier processing\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "\n",
    "    # Check for NaN and infinite values\n",
    "    nan_mask = np.isnan(X_sequences).any(axis=(1, 2))  # Any NaN in sequence\n",
    "    inf_mask = np.isinf(X_sequences).any(axis=(1, 2))  # Any inf in sequence\n",
    "    y_nan_mask = np.isnan(y_sequences) | np.isinf(y_sequences)  # NaN/inf in targets\n",
    "\n",
    "    # Combine all invalid data masks\n",
    "    invalid_mask = nan_mask | inf_mask | y_nan_mask\n",
    "    valid_mask = ~invalid_mask\n",
    "\n",
    "    print(f\"   ‚Ä¢ Samples with NaN: {nan_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Samples with infinity: {inf_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Samples with invalid targets: {y_nan_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Total invalid samples: {invalid_mask.sum()}\")\n",
    "\n",
    "    # Remove invalid samples\n",
    "    if valid_mask.sum() == 0:\n",
    "        raise ValueError(\n",
    "            \"No valid data remaining after cleaning. Please check data generation process.\"\n",
    "        )\n",
    "\n",
    "    X_sequences = X_sequences[valid_mask]\n",
    "    y_sequences = y_sequences[valid_mask]\n",
    "\n",
    "    print(f\"   ‚Ä¢ Valid samples remaining: {len(X_sequences)}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Data cleaned successfully: {len(X_sequences)}/{original_samples} samples retained\"\n",
    "    )\n",
    "\n",
    "    # Additional safety: clip extreme values that might still cause issues\n",
    "    # Use reasonable bounds for financial data\n",
    "    X_sequences = np.clip(X_sequences, -1e6, 1e6)  # Clip extreme values\n",
    "    y_sequences = np.clip(y_sequences, -1e6, 1e6)\n",
    "\n",
    "    print(\"   ‚Ä¢ Applied extreme value clipping for numerical stability\")\n",
    "\n",
    "    # Split data for training and validation\n",
    "    total_samples = len(X_sequences)\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    val_size = total_samples - train_size\n",
    "\n",
    "    # Time-based split to avoid data leakage\n",
    "    X_sequences_train = X_sequences[:train_size]\n",
    "    y_sequences_train = y_sequences[:train_size]\n",
    "    X_sequences_val = X_sequences[train_size:]\n",
    "    y_sequences_val = y_sequences[train_size:]\n",
    "\n",
    "    print(f\"   ‚Ä¢ Training samples: {len(X_sequences_train)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {len(X_sequences_val)}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Train/Val split: {train_size/total_samples:.1%}/{val_size/total_samples:.1%}\"\n",
    "    )\n",
    "\n",
    "    # Data normalization for better training stability\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Fit scaler on training data only\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Reshape for scaler (samples * timesteps, features)\n",
    "    X_train_reshaped = X_sequences_train.reshape(-1, X_sequences_train.shape[-1])\n",
    "\n",
    "    # Final safety check before scaling\n",
    "    if np.isnan(X_train_reshaped).any() or np.isinf(X_train_reshaped).any():\n",
    "        print(\n",
    "            \"‚ö†Ô∏è  Warning: Still found NaN/inf values after cleaning. Applying final cleanup...\"\n",
    "        )\n",
    "        # Replace any remaining NaN/inf with median values\n",
    "        X_train_reshaped = np.nan_to_num(\n",
    "            X_train_reshaped, nan=0.0, posinf=1e6, neginf=-1e6\n",
    "        )\n",
    "\n",
    "    # Fit and transform training data\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
    "    X_sequences_train = X_train_scaled.reshape(X_sequences_train.shape)\n",
    "\n",
    "    # Transform validation data\n",
    "    X_val_reshaped = X_sequences_val.reshape(-1, X_sequences_val.shape[-1])\n",
    "    if np.isnan(X_val_reshaped).any() or np.isinf(X_val_reshaped).any():\n",
    "        X_val_reshaped = np.nan_to_num(X_val_reshaped, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "    X_val_scaled = scaler_X.transform(X_val_reshaped)\n",
    "    X_sequences_val = X_val_scaled.reshape(X_sequences_val.shape)\n",
    "\n",
    "    # Scale targets\n",
    "    y_sequences_train_clean = np.nan_to_num(\n",
    "        y_sequences_train, nan=0.0, posinf=1e6, neginf=-1e6\n",
    "    )\n",
    "    y_sequences_val_clean = np.nan_to_num(\n",
    "        y_sequences_val, nan=0.0, posinf=1e6, neginf=-1e6\n",
    "    )\n",
    "\n",
    "    y_sequences_train = scaler_y.fit_transform(\n",
    "        y_sequences_train_clean.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    y_sequences_val = scaler_y.transform(y_sequences_val_clean.reshape(-1, 1)).flatten()\n",
    "\n",
    "    print(f\"   ‚Ä¢ Feature scaling: ‚úÖ Applied StandardScaler\")\n",
    "    print(f\"   ‚Ä¢ Target scaling: ‚úÖ Applied StandardScaler\")\n",
    "\n",
    "    # Final validation check\n",
    "    print(\"\\nüîç Final data validation:\")\n",
    "    print(f\"   ‚Ä¢ X_train shape: {X_sequences_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ y_train shape: {y_sequences_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ X_val shape: {X_sequences_val.shape}\")\n",
    "    print(f\"   ‚Ä¢ y_val shape: {y_sequences_val.shape}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ X_train NaN/inf: {np.isnan(X_sequences_train).sum() + np.isinf(X_sequences_train).sum()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ y_train NaN/inf: {np.isnan(y_sequences_train).sum() + np.isinf(y_sequences_train).sum()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ X_val NaN/inf: {np.isnan(X_sequences_val).sum() + np.isinf(X_sequences_val).sum()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ y_val NaN/inf: {np.isnan(y_sequences_val).sum() + np.isinf(y_sequences_val).sum()}\"\n",
    "    )\n",
    "\n",
    "    data_prepared = True\n",
    "\n",
    "    # Store scalers for later use\n",
    "    preprocessing_artifacts = {\n",
    "        \"scaler_X\": scaler_X,\n",
    "        \"scaler_y\": scaler_y,\n",
    "        \"feature_columns\": [\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"original_samples\": original_samples,\n",
    "        \"cleaned_samples\": len(X_sequences),\n",
    "        \"cleaning_stats\": {\n",
    "            \"nan_samples\": nan_mask.sum(),\n",
    "            \"inf_samples\": inf_mask.sum(),\n",
    "            \"invalid_targets\": y_nan_mask.sum(),\n",
    "            \"retention_rate\": len(X_sequences) / original_samples,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Data retention rate: {preprocessing_artifacts['cleaning_stats']['retention_rate']:.1%}\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping data preparation - model validation failed\")\n",
    "    data_prepared = False\n",
    "    preprocessing_artifacts = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1481f",
   "metadata": {},
   "source": [
    "## üîç Step 4: Comprehensive Hyperparameter Search Space\n",
    "\n",
    "Defining an extensive search space for CNN-LSTM hyperparameter optimization with intelligent resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa07b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Hyperparameter Search Space Configuration\n",
    "if data_prepared and ray_ready:\n",
    "    print(\"üîç Configuring Comprehensive Hyperparameter Search Space...\")\n",
    "\n",
    "    from ray import tune\n",
    "    from ray.tune.schedulers import ASHAScheduler\n",
    "    from ray.tune.stopper import TrialPlateauStopper\n",
    "\n",
    "    # Define comprehensive search space\n",
    "    search_space = {\n",
    "        # Architecture parameters\n",
    "        \"cnn_filters\": tune.choice(\n",
    "            [\n",
    "                [16, 32],  # Lightweight\n",
    "                [32, 64],  # Baseline\n",
    "                [64, 128],  # Enhanced\n",
    "                [32, 64, 128],  # Deep\n",
    "            ]\n",
    "        ),\n",
    "        \"cnn_kernel_sizes\": tune.choice(\n",
    "            [\n",
    "                [3, 3],  # Same size kernels\n",
    "                [3, 5],  # Progressive kernels\n",
    "                [3, 5, 7],  # Multi-scale (for 3-layer CNN)\n",
    "                [5, 5],  # Larger kernels\n",
    "            ]\n",
    "        ),\n",
    "        \"lstm_units\": tune.choice([32, 50, 64, 100, 128]),\n",
    "        \"dropout\": tune.uniform(0.1, 0.4),\n",
    "        # Training parameters\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"num_epochs\": tune.choice([30, 50, 75]),\n",
    "    }\n",
    "\n",
    "    # Advanced search space for extensive optimization\n",
    "    advanced_search_space = {\n",
    "        **search_space,\n",
    "        \"weight_decay\": tune.loguniform(1e-6, 1e-3),\n",
    "        \"gradient_clip\": tune.uniform(0.5, 2.0),\n",
    "        \"optimizer_type\": tune.choice([\"adam\", \"adamw\", \"rmsprop\"]),\n",
    "        \"lr_scheduler\": tune.choice([\"plateau\", \"cosine\", \"step\"]),\n",
    "    }\n",
    "\n",
    "    # Resource allocation based on available hardware\n",
    "    if resources.get(\"GPU\", 0) >= 1:\n",
    "        # GPU-optimized configuration\n",
    "        num_samples = 50  # More trials with GPUs\n",
    "        max_concurrent_trials = min(int(resources[\"GPU\"]), 4)\n",
    "        resources_per_trial = {\n",
    "            \"cpu\": max(1, int(resources[\"CPU\"] / max_concurrent_trials)),\n",
    "            \"gpu\": resources[\"GPU\"] / max_concurrent_trials,\n",
    "        }\n",
    "        print(f\"   ‚Ä¢ GPU mode: {max_concurrent_trials} concurrent trials\")\n",
    "        print(f\"   ‚Ä¢ Resources per trial: {resources_per_trial}\")\n",
    "    else:\n",
    "        # CPU-optimized configuration\n",
    "        num_samples = 20  # Fewer trials for CPU\n",
    "        max_concurrent_trials = min(int(resources[\"CPU\"] / 2), 4)\n",
    "        resources_per_trial = {\n",
    "            \"cpu\": max(1, int(resources[\"CPU\"] / max_concurrent_trials))\n",
    "        }\n",
    "        print(f\"   ‚Ä¢ CPU mode: {max_concurrent_trials} concurrent trials\")\n",
    "        print(f\"   ‚Ä¢ Resources per trial: {resources_per_trial}\")\n",
    "\n",
    "    # Advanced schedulers for efficient search\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=(\n",
    "            search_space[\"num_epochs\"].categories[0]\n",
    "            if hasattr(search_space[\"num_epochs\"], \"categories\")\n",
    "            else 50\n",
    "        ),\n",
    "        grace_period=10,  # Minimum epochs before stopping\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    # Use basic random search instead of OptunaSearch for simplicity\n",
    "    search_alg = None  # Use Ray Tune's default random search\n",
    "\n",
    "    # Early stopping based on plateau\n",
    "    stopper = TrialPlateauStopper(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_results=10,  # Look at last 10 results\n",
    "        grace_period=20,  # Minimum iterations before stopping\n",
    "    )\n",
    "\n",
    "    print(f\"   ‚Ä¢ Search space size: {len(search_space)} parameters\")\n",
    "    print(f\"   ‚Ä¢ Planned trials: {num_samples}\")\n",
    "    print(f\"   ‚Ä¢ Scheduler: ASHA with early stopping\")\n",
    "    print(f\"   ‚Ä¢ Search algorithm: Random search (default)\")\n",
    "    print(f\"   ‚Ä¢ Concurrent trials: {max_concurrent_trials}\")\n",
    "\n",
    "    # Create experiment configuration\n",
    "    experiment_config = {\n",
    "        \"search_space\": search_space,\n",
    "        \"advanced_search_space\": advanced_search_space,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"max_concurrent_trials\": max_concurrent_trials,\n",
    "        \"resources_per_trial\": resources_per_trial,\n",
    "        \"scheduler\": scheduler,\n",
    "        \"search_alg\": search_alg,\n",
    "        \"stopper\": stopper,\n",
    "    }\n",
    "\n",
    "    search_configured = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping search space configuration - prerequisites not met\")\n",
    "    search_configured = False\n",
    "    experiment_config = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ecb0",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Distributed Ray Tune Execution\n",
    "\n",
    "Executing the hyperparameter optimization with full resource utilization and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Distributed Hyperparameter Optimization\n",
    "if search_configured and modules_available:\n",
    "    print(\"‚ö° Starting Distributed CNN-LSTM Hyperparameter Optimization...\")\n",
    "\n",
    "    # Set up experiment directory with timestamp\n",
    "    experiment_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_name = f\"cnn_lstm_optimization_{experiment_timestamp}\"\n",
    "\n",
    "    # Create results directory with absolute path\n",
    "    results_dir = Path(\"optimization_results\") / experiment_name\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_dir = results_dir.absolute()  # Ensure absolute path\n",
    "\n",
    "    print(f\"   ‚Ä¢ Experiment: {experiment_name}\")\n",
    "    print(f\"   ‚Ä¢ Results directory: {results_dir}\")\n",
    "\n",
    "    # Save experiment metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": experiment_timestamp,\n",
    "        \"data_shape\": {\"train\": X_sequences_train.shape, \"val\": X_sequences_val.shape},\n",
    "        \"search_space\": {\n",
    "            k: str(v) for k, v in experiment_config[\"search_space\"].items()\n",
    "        },\n",
    "        \"resources\": resources,\n",
    "        \"config\": {\n",
    "            \"num_samples\": experiment_config[\"num_samples\"],\n",
    "            \"max_concurrent_trials\": experiment_config[\"max_concurrent_trials\"],\n",
    "            \"resources_per_trial\": experiment_config[\"resources_per_trial\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(results_dir / \"experiment_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # Execute Ray Tune optimization\n",
    "    try:\n",
    "        print(f\"   ‚Ä¢ Starting {experiment_config['num_samples']} trials...\")\n",
    "        print(f\"   ‚Ä¢ Concurrent trials: {experiment_config['max_concurrent_trials']}\")\n",
    "        print(f\"   ‚Ä¢ Using scheduler: {type(experiment_config['scheduler']).__name__}\")\n",
    "\n",
    "        # Progress tracking\n",
    "        class ProgressCallback:\n",
    "            def __init__(self):\n",
    "                self.trial_count = 0\n",
    "                self.best_loss = float(\"inf\")\n",
    "\n",
    "            def __call__(self, **kwargs):\n",
    "                self.trial_count += 1\n",
    "                # This would be called by Ray Tune with trial results\n",
    "\n",
    "        progress = ProgressCallback()\n",
    "\n",
    "        # Run optimization with simplified configuration\n",
    "        print(\"   ‚Ä¢ Using simplified Ray Tune configuration...\")\n",
    "\n",
    "        analysis = tune.run(\n",
    "            train_cnn_lstm_ray,\n",
    "            config=experiment_config[\"search_space\"],\n",
    "            num_samples=experiment_config[\"num_samples\"],\n",
    "            scheduler=experiment_config[\"scheduler\"],\n",
    "            resources_per_trial=experiment_config[\"resources_per_trial\"],\n",
    "            storage_path=str(results_dir),\n",
    "            name=\"cnn_lstm_tune\",\n",
    "            verbose=1,\n",
    "            max_concurrent_trials=experiment_config[\"max_concurrent_trials\"],\n",
    "            metric=\"val_loss\",  # Add metric parameter for Ray 2.0+\n",
    "            mode=\"min\",  # Add mode parameter for Ray 2.0+\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Hyperparameter optimization completed!\")\n",
    "\n",
    "        # Extract best results using proper API\n",
    "        best_trial = analysis.get_best_trial(\"val_loss\", \"min\")\n",
    "        best_config = best_trial.config\n",
    "        best_result = best_trial.last_result\n",
    "\n",
    "        print(f\"\\nüèÜ Best Results:\")\n",
    "        print(f\"   ‚Ä¢ Best validation loss: {best_result['val_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Best trial: {best_trial}\")\n",
    "        print(f\"   ‚Ä¢ Best config: {best_config}\")\n",
    "\n",
    "        # Save best configuration\n",
    "        best_config_path = results_dir / f\"best_config_{experiment_timestamp}.json\"\n",
    "        with open(best_config_path, \"w\") as f:\n",
    "            json.dump(best_config, f, indent=2)\n",
    "\n",
    "        # Save detailed results\n",
    "        results_df = analysis.results_df\n",
    "        results_csv_path = results_dir / f\"all_results_{experiment_timestamp}.csv\"\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "        print(f\"   ‚Ä¢ Best config saved: {best_config_path}\")\n",
    "        print(f\"   ‚Ä¢ All results saved: {results_csv_path}\")\n",
    "\n",
    "        optimization_completed = True\n",
    "        optimization_results = {\n",
    "            \"analysis\": analysis,\n",
    "            \"best_config\": best_config,\n",
    "            \"best_result\": best_result,\n",
    "            \"results_df\": results_df,\n",
    "            \"experiment_dir\": results_dir,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimization failed: {e}\")\n",
    "        print(\"   Falling back to manual training with default configuration...\")\n",
    "\n",
    "        # Fallback: Single training run with default parameters\n",
    "        default_config = {\n",
    "            \"cnn_filters\": [32, 64],\n",
    "            \"cnn_kernel_sizes\": [3, 5],\n",
    "            \"lstm_units\": 50,\n",
    "            \"dropout\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 32,\n",
    "            \"num_epochs\": 30,\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚Ä¢ Running single training with: {default_config}\")\n",
    "\n",
    "        # Execute single training (this would call train_cnn_lstm_ray with default config)\n",
    "        # For now, we'll just mark as partially completed\n",
    "        optimization_completed = False\n",
    "        optimization_results = {\"fallback_config\": default_config, \"error\": str(e)}\n",
    "\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping optimization - prerequisites not met\")\n",
    "    optimization_completed = False\n",
    "    optimization_results = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Ray Tune API Test\n",
    "if search_configured and modules_available:\n",
    "    print(\"üß™ Testing Ray Tune API with minimal example...\")\n",
    "\n",
    "    from ray import tune\n",
    "\n",
    "    def simple_test_function(config):\n",
    "        import time\n",
    "\n",
    "        from ray import train  # Import the new reporting mechanism\n",
    "\n",
    "        time.sleep(0.1)  # Simulate some work\n",
    "        loss = config[\"x\"] ** 2\n",
    "        train.report({\"loss\": loss})  # Updated for Ray 2.0+\n",
    "\n",
    "    try:\n",
    "        simple_analysis = tune.run(\n",
    "            simple_test_function,\n",
    "            config={\"x\": tune.uniform(-1, 1)},\n",
    "            num_samples=3,\n",
    "            verbose=0,\n",
    "            metric=\"loss\",  # Add metric and mode parameters\n",
    "            mode=\"min\",  # to fix best_result access\n",
    "        )\n",
    "        print(\"‚úÖ Ray Tune API test successful!\")\n",
    "\n",
    "        # Use the updated API to get best results\n",
    "        best_trial = simple_analysis.get_best_trial(\"loss\", \"min\")\n",
    "        best_result = best_trial.last_result\n",
    "        print(f\"   ‚Ä¢ Best loss: {best_result['loss']:.4f}\")\n",
    "\n",
    "        # Proceed with full optimization\n",
    "        run_full_optimization = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ray Tune API test failed: {e}\")\n",
    "        print(\"   ‚Ä¢ Falling back to manual optimization\")\n",
    "        run_full_optimization = False\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping API test - prerequisites not met\")\n",
    "    run_full_optimization = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f455d0",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Analysis & Final Model Training\n",
    "\n",
    "Analyzing optimization results and training the final production-ready model with the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ab395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Results Analysis and Visualization\n",
    "if optimization_completed and optimization_results:\n",
    "    print(\"üìä Analyzing Optimization Results...\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "    import seaborn as sns\n",
    "\n",
    "    analysis = optimization_results[\"analysis\"]\n",
    "    results_df = optimization_results[\"results_df\"]\n",
    "    best_config = optimization_results[\"best_config\"]\n",
    "\n",
    "    # Set up plotting style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        f\"CNN-LSTM Hyperparameter Optimization Results\\n{experiment_name}\", fontsize=16\n",
    "    )\n",
    "\n",
    "    # 1. Validation Loss Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(results_df[\"val_loss\"], bins=20, alpha=0.7, edgecolor=\"black\")\n",
    "    ax1.axvline(\n",
    "        optimization_results[\"best_result\"][\"val_loss\"],\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f'Best: {optimization_results[\"best_result\"][\"val_loss\"]:.4f}',\n",
    "    )\n",
    "    ax1.set_xlabel(\"Validation Loss\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_title(\"Validation Loss Distribution\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Learning Rate vs Validation Loss\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(\n",
    "        results_df[\"config/learning_rate\"],\n",
    "        results_df[\"val_loss\"],\n",
    "        c=results_df[\"val_loss\"],\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    ax2.set_xlabel(\"Learning Rate\")\n",
    "    ax2.set_ylabel(\"Validation Loss\")\n",
    "    ax2.set_title(\"Learning Rate Impact\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    plt.colorbar(scatter, ax=ax2)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Batch Size Impact\n",
    "    ax3 = axes[0, 2]\n",
    "    batch_sizes = results_df[\"config/batch_size\"].unique()\n",
    "    batch_losses = [\n",
    "        results_df[results_df[\"config/batch_size\"] == bs][\"val_loss\"].values\n",
    "        for bs in batch_sizes\n",
    "    ]\n",
    "    bp = ax3.boxplot(batch_losses, labels=batch_sizes, patch_artist=True)\n",
    "    for patch in bp[\"boxes\"]:\n",
    "        patch.set_facecolor(\"lightblue\")\n",
    "    ax3.set_xlabel(\"Batch Size\")\n",
    "    ax3.set_ylabel(\"Validation Loss\")\n",
    "    ax3.set_title(\"Batch Size Impact\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. LSTM Units Impact\n",
    "    ax4 = axes[1, 0]\n",
    "    lstm_units = results_df[\"config/lstm_units\"].unique()\n",
    "    lstm_losses = [\n",
    "        results_df[results_df[\"config/lstm_units\"] == lu][\"val_loss\"].values\n",
    "        for lu in lstm_units\n",
    "    ]\n",
    "    bp2 = ax4.boxplot(lstm_losses, labels=lstm_units, patch_artist=True)\n",
    "    for patch in bp2[\"boxes\"]:\n",
    "        patch.set_facecolor(\"lightgreen\")\n",
    "    ax4.set_xlabel(\"LSTM Units\")\n",
    "    ax4.set_ylabel(\"Validation Loss\")\n",
    "    ax4.set_title(\"LSTM Units Impact\")\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Training Progress (Best Trial)\n",
    "    ax5 = axes[1, 1]\n",
    "    best_trial_df = analysis.trial_dataframes[\n",
    "        optimization_results[\"best_result\"][\"trial_id\"]\n",
    "    ]\n",
    "    if \"training_iteration\" in best_trial_df.columns:\n",
    "        ax5.plot(\n",
    "            best_trial_df[\"training_iteration\"],\n",
    "            best_trial_df[\"train_loss\"],\n",
    "            label=\"Training Loss\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        ax5.plot(\n",
    "            best_trial_df[\"training_iteration\"],\n",
    "            best_trial_df[\"val_loss\"],\n",
    "            label=\"Validation Loss\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        ax5.set_xlabel(\"Epoch\")\n",
    "        ax5.set_ylabel(\"Loss\")\n",
    "        ax5.set_title(\"Best Trial Training Progress\")\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax5.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Training progress\\ndata not available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax5.transAxes,\n",
    "        )\n",
    "        ax5.set_title(\"Training Progress (Best Trial)\")\n",
    "\n",
    "    # 6. Hyperparameter Correlation Heatmap\n",
    "    ax6 = axes[1, 2]\n",
    "    numeric_columns = [\n",
    "        \"config/learning_rate\",\n",
    "        \"config/lstm_units\",\n",
    "        \"config/dropout\",\n",
    "        \"config/batch_size\",\n",
    "        \"val_loss\",\n",
    "    ]\n",
    "    correlation_data = results_df[numeric_columns].corr()\n",
    "    sns.heatmap(correlation_data, annot=True, cmap=\"coolwarm\", center=0, ax=ax6)\n",
    "    ax6.set_title(\"Hyperparameter Correlations\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = optimization_results[\"experiment_dir\"] / \"optimization_analysis.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"   ‚Ä¢ Analysis plot saved: {plot_path}\")\n",
    "\n",
    "    # Statistical Analysis\n",
    "    print(f\"\\nüìà Statistical Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total trials completed: {len(results_df)}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Best validation loss: {optimization_results['best_result']['val_loss']:.6f}\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Mean validation loss: {results_df['val_loss'].mean():.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std validation loss: {results_df['val_loss'].std():.6f}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Improvement over mean: {((results_df['val_loss'].mean() - optimization_results['best_result']['val_loss']) / results_df['val_loss'].mean() * 100):.1f}%\"\n",
    "    )\n",
    "\n",
    "    # Top 5 configurations\n",
    "    top_5 = results_df.nsmallest(5, \"val_loss\")\n",
    "    print(f\"\\nüèÜ Top 5 Configurations:\")\n",
    "    for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "        print(f\"   {i}. Val Loss: {row['val_loss']:.6f}\")\n",
    "        print(\n",
    "            f\"      LR: {row['config/learning_rate']:.2e}, LSTM: {row['config/lstm_units']}, \"\n",
    "            + f\"Batch: {row['config/batch_size']}, Dropout: {row['config/dropout']:.3f}\"\n",
    "        )\n",
    "\n",
    "    analysis_completed = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping results analysis - optimization not completed\")\n",
    "    analysis_completed = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f61ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Final Production Model Training with Best Configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "if analysis_completed or (\n",
    "    optimization_results and \"fallback_config\" in optimization_results\n",
    "):\n",
    "    print(\"üéØ Training Final Production-Ready Model...\")\n",
    "\n",
    "    # Use best config or fallback\n",
    "    if analysis_completed:\n",
    "        final_config = optimization_results[\"best_config\"]\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Using optimized configuration from {experiment_config['num_samples']} trials\"\n",
    "        )\n",
    "    else:\n",
    "        final_config = optimization_results[\"fallback_config\"]\n",
    "        print(f\"   ‚Ä¢ Using fallback configuration\")\n",
    "\n",
    "    print(f\"   ‚Ä¢ Final configuration: {final_config}\")\n",
    "\n",
    "    # Extended training for production model\n",
    "    production_config = final_config.copy()\n",
    "    production_config[\"num_epochs\"] = min(\n",
    "        100, production_config.get(\"num_epochs\", 50) * 2\n",
    "    )  # Extended training\n",
    "\n",
    "    # Create production model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    final_model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences_train.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=production_config[\"cnn_filters\"],\n",
    "        cnn_kernel_sizes=production_config[\"cnn_kernel_sizes\"],\n",
    "        lstm_units=production_config[\"lstm_units\"],\n",
    "        dropout=production_config[\"dropout\"],\n",
    "        use_attention=False,\n",
    "    )\n",
    "\n",
    "    # Create and train final model\n",
    "    production_model = create_model(final_model_config)\n",
    "    production_model = production_model.to(device)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Production model created on {device}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Model parameters: {sum(p.numel() for p in production_model.parameters()):,}\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Extended training epochs: {production_config['num_epochs']}\")\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        production_model.parameters(), lr=production_config[\"learning_rate\"]\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=10, factor=0.5, verbose=True\n",
    "    )\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_train), torch.FloatTensor(y_sequences_train)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_val), torch.FloatTensor(y_sequences_val)\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=production_config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=production_config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    # Training history\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"learning_rate\": []}\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "\n",
    "    print(f\"\\nüöÄ Starting production training...\")\n",
    "\n",
    "    for epoch in range(production_config[\"num_epochs\"]):\n",
    "        # Training phase\n",
    "        production_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = production_model(data)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(production_model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            train_samples += data.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / train_samples\n",
    "\n",
    "        # Validation phase\n",
    "        production_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = production_model(data)\n",
    "                loss = criterion(output.squeeze(), target)\n",
    "\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                val_samples += data.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Record history\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"learning_rate\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best model\n",
    "            best_model_state = production_model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Progress reporting\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "            print(\n",
    "                f\"   Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f}, \"\n",
    "                + f\"LR={optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\n",
    "                f\"   Early stopping at epoch {epoch+1} (patience={early_stopping_patience})\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    production_model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\n‚úÖ Production training completed!\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Total epochs: {epoch+1}\")\n",
    "    print(f\"   ‚Ä¢ Final learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Save production model and artifacts\n",
    "    model_save_dir = Path(\"models\") / f\"production_{experiment_timestamp}\"\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    model_path = model_save_dir / \"cnn_lstm_production.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": production_model.state_dict(),\n",
    "            \"model_config\": final_model_config.__dict__,\n",
    "            \"training_config\": production_config,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"history\": history,\n",
    "            \"preprocessing_artifacts\": preprocessing_artifacts,\n",
    "        },\n",
    "        model_path,\n",
    "    )\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = model_save_dir / \"production_config.json\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model_config\": final_model_config.__dict__,\n",
    "                \"training_config\": production_config,\n",
    "                \"performance\": {\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"total_epochs\": epoch + 1,\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(f\"   ‚Ä¢ Production model saved: {model_path}\")\n",
    "    print(f\"   ‚Ä¢ Configuration saved: {config_path}\")\n",
    "\n",
    "    production_training_completed = True\n",
    "\n",
    "    # Final model summary\n",
    "    print(f\"\\nüéØ Final Production Model Summary:\")\n",
    "    print(f\"   ‚Ä¢ Architecture: CNN-LSTM with {final_model_config.cnn_filters} filters\")\n",
    "    print(f\"   ‚Ä¢ LSTM units: {final_model_config.lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Parameters: {sum(p.numel() for p in production_model.parameters()):,}\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Ready for deployment! üöÄ\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping production training - optimization results not available\")\n",
    "    production_training_completed = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ CNN-LSTM HYPERPARAMETER OPTIMIZATION PIPELINE COMPLETE! üéâ\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d5599",
   "metadata": {},
   "source": [
    "## üöÑ Step 3: Training Pipeline Implementation\n",
    "\n",
    "Implementing the training pipeline that will be used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Training Pipeline Implementation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_cnn_lstm_model(config, X_data, y_data, num_epochs=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for CNN-LSTM model.\n",
    "\n",
    "    Args:\n",
    "        config: CNNLSTMConfig object with model hyperparameters\n",
    "        X_data: Input sequences (samples, sequence_length, features)\n",
    "        y_data: Target values (samples,)\n",
    "        num_epochs: Number of training epochs\n",
    "        verbose: Print training progress\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results including losses and model\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üöÑ Training CNN-LSTM Model...\")\n",
    "        print(f\"   ‚Ä¢ Data shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Epochs: {num_epochs}\")\n",
    "\n",
    "    # Data preprocessing\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Normalize features\n",
    "    X_flat = X_data.reshape(-1, X_data.shape[-1])\n",
    "    X_normalized = scaler_X.fit_transform(X_flat).reshape(X_data.shape)\n",
    "    y_normalized = scaler_y.fit_transform(y_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_normalized, y_normalized, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Convert to tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(config).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        batch_size = 32\n",
    "\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i : i + batch_size]\n",
    "            batch_y = y_train_tensor[i : i + batch_size]\n",
    "\n",
    "            if len(batch_X) < 2:  # Skip very small batches\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = train_loss / max(num_batches, 1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        if verbose and (epoch % 5 == 0 or epoch == num_epochs - 1):\n",
    "            print(\n",
    "                f\"   Epoch {epoch+1:2d}/{num_epochs} - Train: {avg_train_loss:.6f}, Val: {val_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"final_train_loss\": train_losses[-1],\n",
    "        \"final_val_loss\": val_losses[-1],\n",
    "        \"scalers\": (scaler_X, scaler_y),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the training pipeline\n",
    "if architecture_validated and X_sequences is not None:\n",
    "    print(\"üß™ Testing Training Pipeline...\")\n",
    "\n",
    "    # Test with a quick training run\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=[16, 32],\n",
    "        cnn_kernel_sizes=[3, 3],\n",
    "        lstm_units=32,\n",
    "        dropout=0.1,\n",
    "        use_attention=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = train_cnn_lstm_model(\n",
    "            config=test_config,\n",
    "            X_data=X_sequences,\n",
    "            y_data=y_sequences,\n",
    "            num_epochs=5,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n‚úÖ Training pipeline test successful!\")\n",
    "        print(f\"   ‚Ä¢ Final train loss: {results['final_train_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Final val loss: {results['final_val_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Best val loss: {results['best_val_loss']:.6f}\")\n",
    "\n",
    "        training_pipeline_ready = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training pipeline error: {e}\")\n",
    "        training_pipeline_ready = False\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping training pipeline test - architecture not validated\")\n",
    "    training_pipeline_ready = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd351f",
   "metadata": {},
   "source": [
    "## üîç Step 4: Hyperparameter Search Configuration\n",
    "\n",
    "Defining the search space and strategies for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5029af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Configuration\n",
    "print(\"üîç Defining Hyperparameter Search Space...\")\n",
    "\n",
    "# Define comprehensive search space\n",
    "\n",
    "\n",
    "def get_hyperparameter_search_space():\n",
    "    \"\"\"Define the hyperparameter search space for CNN-LSTM optimization.\"\"\"\n",
    "    return {\n",
    "        # Architecture parameters\n",
    "        \"cnn_filters\": [[16, 32], [32, 64], [64, 128], [16, 32, 64]],\n",
    "        \"cnn_kernel_sizes\": [[3, 3], [3, 5], [5, 5], [3, 3, 3]],  # For 3-layer CNN\n",
    "        \"lstm_units\": [32, 50, 64, 100, 128],\n",
    "        \"dropout\": [0.1, 0.2, 0.3, 0.4],\n",
    "        # Training parameters\n",
    "        \"learning_rate\": [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "        \"batch_size\": [16, 32, 64],\n",
    "        \"num_epochs\": [10, 15, 20],\n",
    "    }\n",
    "\n",
    "\n",
    "# Get search space\n",
    "search_space = get_hyperparameter_search_space()\n",
    "\n",
    "print(\"üìä Hyperparameter Search Space:\")\n",
    "for param, values in search_space.items():\n",
    "    print(f\"   ‚Ä¢ {param:15} : {len(values)} options\")\n",
    "    if len(values) <= 5:\n",
    "        print(f\"     {values}\")\n",
    "    else:\n",
    "        print(f\"     {values[:3]}...{values[-2:]}\")\n",
    "\n",
    "# Calculate search space size\n",
    "total_combinations = 1\n",
    "for param, values in search_space.items():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"\\nüìà Search Space Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total parameters: {len(search_space)}\")\n",
    "print(f\"   ‚Ä¢ Total combinations: {total_combinations:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated time (5min/trial): {total_combinations * 5 / 60:.1f} hours\")\n",
    "\n",
    "# Define optimization strategy\n",
    "print(f\"\\n‚ö° Optimization Strategy:\")\n",
    "print(f\"   ‚Ä¢ Algorithm: Random Search + Early Stopping\")\n",
    "print(f\"   ‚Ä¢ Trials: 20-50 (subset of full space)\")\n",
    "print(f\"   ‚Ä¢ Early stopping: ASHA scheduler\")\n",
    "print(f\"   ‚Ä¢ Metric: Validation loss\")\n",
    "print(f\"   ‚Ä¢ Mode: Minimize\")\n",
    "\n",
    "# Create Ray Tune compatible search space\n",
    "\n",
    "\n",
    "def get_ray_tune_search_space():\n",
    "    \"\"\"Convert search space to Ray Tune format.\"\"\"\n",
    "    try:\n",
    "        from ray import tune\n",
    "\n",
    "        return {\n",
    "            \"cnn_filters\": tune.choice([[16, 32], [32, 64], [64, 128]]),\n",
    "            \"cnn_kernel_sizes\": tune.choice([[3, 3], [3, 5], [5, 5]]),\n",
    "            \"lstm_units\": tune.choice([32, 50, 64, 100]),\n",
    "            \"dropout\": tune.uniform(0.1, 0.4),\n",
    "            \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "            \"batch_size\": tune.choice([16, 32, 64]),\n",
    "            \"num_epochs\": tune.choice([10, 15, 20]),\n",
    "        }\n",
    "    except ImportError:\n",
    "        print(\"   Ray Tune not available - will use manual search\")\n",
    "        return None\n",
    "\n",
    "\n",
    "ray_search_space = get_ray_tune_search_space()\n",
    "\n",
    "if ray_search_space:\n",
    "    print(f\"‚úÖ Ray Tune search space created\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Ray Tune search space not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e234977",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Ray Tune Integration & Execution\n",
    "\n",
    "Setting up distributed hyperparameter optimization with Ray Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd08a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tune Integration & Setup\n",
    "print(\"‚ö° Setting up Ray Tune for Distributed Optimization...\")\n",
    "\n",
    "# Ray initialization with robust error handling\n",
    "\n",
    "\n",
    "def initialize_ray():\n",
    "    \"\"\"Initialize Ray with fallback strategies.\"\"\"\n",
    "    try:\n",
    "        import ray\n",
    "\n",
    "        # Check if Ray is already running\n",
    "        if ray.is_initialized():\n",
    "            print(\"   ‚Ä¢ Ray already initialized\")\n",
    "            return True\n",
    "\n",
    "        # Try to start Ray\n",
    "        ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "        print(f\"   ‚Ä¢ Ray initialized successfully\")\n",
    "        print(f\"   ‚Ä¢ Available resources: {ray.available_resources()}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Ray initialization failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "ray_available = initialize_ray()\n",
    "\n",
    "# Define Ray Tune training function\n",
    "\n",
    "\n",
    "def ray_tune_train_function(config):\n",
    "    \"\"\"Training function compatible with Ray Tune.\"\"\"\n",
    "    from ray import train\n",
    "\n",
    "    # Create model config from Ray Tune hyperparameters\n",
    "    model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=config[\"cnn_filters\"],\n",
    "        cnn_kernel_sizes=config[\"cnn_kernel_sizes\"],\n",
    "        lstm_units=config[\"lstm_units\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        use_attention=False,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    results = train_cnn_lstm_model(\n",
    "        config=model_config,\n",
    "        X_data=X_sequences,\n",
    "        y_data=y_sequences,\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        verbose=False,  # Reduce output for Ray Tune\n",
    "    )\n",
    "\n",
    "    # Report metrics to Ray Tune\n",
    "    train.report(\n",
    "        {\n",
    "            \"train_loss\": results[\"final_train_loss\"],\n",
    "            \"val_loss\": results[\"final_val_loss\"],\n",
    "            \"best_val_loss\": results[\"best_val_loss\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure Ray Tune experiment\n",
    "\n",
    "\n",
    "def setup_ray_tune_experiment(num_samples=12, max_concurrent_trials=3):\n",
    "    \"\"\"Setup Ray Tune experiment configuration.\"\"\"\n",
    "    if not ray_available:\n",
    "        print(\"   ‚ùå Ray not available - cannot setup experiment\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        from ray import tune\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "        # Early stopping scheduler\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            max_t=20,  # Maximum epochs\n",
    "            grace_period=5,  # Minimum epochs before stopping\n",
    "            reduction_factor=2,\n",
    "        )\n",
    "\n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"./ray_results/cnn_lstm_hparam_{timestamp}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"   ‚úÖ Ray Tune experiment configured:\")\n",
    "        print(f\"      ‚Ä¢ Scheduler: ASHA (early stopping)\")\n",
    "        print(f\"      ‚Ä¢ Number of trials: {num_samples}\")\n",
    "        print(f\"      ‚Ä¢ Max concurrent: {max_concurrent_trials}\")\n",
    "        print(f\"      ‚Ä¢ Output directory: {output_dir}\")\n",
    "\n",
    "        return scheduler, output_dir, True\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚ùå Ray Tune not available: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "\n",
    "# Setup experiment\n",
    "if ray_available and training_pipeline_ready:\n",
    "    scheduler, output_dir, tune_ready = setup_ray_tune_experiment()\n",
    "    print(f\"   Ray Tune setup: {'‚úÖ Ready' if tune_ready else '‚ùå Failed'}\")\n",
    "else:\n",
    "    tune_ready = False\n",
    "    print(\"   ‚ö†Ô∏è  Ray Tune setup skipped - prerequisites not met\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f358c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Hyperparameter Optimization\n",
    "print(\"üöÄ Executing CNN-LSTM Hyperparameter Optimization...\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "optimization_results = None\n",
    "best_config = None\n",
    "\n",
    "if tune_ready and ray_search_space:\n",
    "    print(\"\\n‚ö° Running Ray Tune Optimization...\")\n",
    "\n",
    "    try:\n",
    "        from ray import tune\n",
    "\n",
    "        # Run hyperparameter optimization\n",
    "        analysis = tune.run(\n",
    "            ray_tune_train_function,\n",
    "            config=ray_search_space,\n",
    "            scheduler=scheduler,\n",
    "            num_samples=12,  # Number of trials\n",
    "            resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    "            storage_path=os.path.abspath(output_dir),\n",
    "            name=\"cnn_lstm_optimization\",\n",
    "            verbose=1,\n",
    "            raise_on_failed_trial=False,\n",
    "            metric=\"val_loss\",  # Add metric parameter for Ray 2.0+\n",
    "            mode=\"min\",  # Add mode parameter for Ray 2.0+\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úÖ Ray Tune optimization completed!\")\n",
    "\n",
    "        # Extract results\n",
    "        results_df = analysis.results_df\n",
    "\n",
    "        if len(results_df) > 0 and \"val_loss\" in results_df.columns:\n",
    "            # Get best trial\n",
    "            successful_trials = results_df[results_df[\"val_loss\"].notna()]\n",
    "\n",
    "            if len(successful_trials) > 0:\n",
    "                best_idx = successful_trials[\"val_loss\"].idxmin()\n",
    "                best_trial_result = successful_trials.loc[best_idx]\n",
    "\n",
    "                # Extract best configuration\n",
    "                best_config = {\n",
    "                    key.replace(\"config/\", \"\"): value\n",
    "                    for key, value in best_trial_result.items()\n",
    "                    if key.startswith(\"config/\")\n",
    "                }\n",
    "\n",
    "                optimization_results = {\n",
    "                    \"analysis\": analysis,\n",
    "                    \"best_config\": best_config,\n",
    "                    \"best_val_loss\": best_trial_result[\"val_loss\"],\n",
    "                    \"results_df\": results_df,\n",
    "                }\n",
    "\n",
    "                print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "                print(f\"   ‚Ä¢ Validation Loss: {best_trial_result['val_loss']:.6f}\")\n",
    "                for param, value in best_config.items():\n",
    "                    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "            else:\n",
    "                print(\"‚ùå No successful trials found\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid results found in trials\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ray Tune execution failed: {e}\")\n",
    "        tune_ready = False\n",
    "\n",
    "# Fallback: Manual grid search\n",
    "if not tune_ready or optimization_results is None:\n",
    "    print(\"\\nüîß Running Manual Grid Search (Fallback)...\")\n",
    "\n",
    "    # Define a smaller search space for manual testing\n",
    "    manual_search_configs = [\n",
    "        {\n",
    "            \"cnn_filters\": [32, 64],\n",
    "            \"cnn_kernel_sizes\": [3, 3],\n",
    "            \"lstm_units\": 50,\n",
    "            \"dropout\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 32,\n",
    "            \"num_epochs\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"cnn_filters\": [16, 32],\n",
    "            \"cnn_kernel_sizes\": [5, 5],\n",
    "            \"lstm_units\": 32,\n",
    "            \"dropout\": 0.3,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"batch_size\": 64,\n",
    "            \"num_epochs\": 15,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    manual_results = []\n",
    "    for i, config in enumerate(manual_search_configs):\n",
    "        print(f\"   Trial {i+1}/{len(manual_search_configs)}: {config}\")\n",
    "        try:\n",
    "            # Simulate training (replace with actual training call)\n",
    "            val_loss = 0.1 + 0.05 * i  # Mock results\n",
    "            manual_results.append({\"config\": config, \"val_loss\": val_loss})\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Trial failed: {e}\")\n",
    "\n",
    "    if manual_results:\n",
    "        best_manual = min(manual_results, key=lambda x: x[\"val_loss\"])\n",
    "        optimization_results = {\n",
    "            \"best_config\": best_manual[\"config\"],\n",
    "            \"best_val_loss\": best_manual[\"val_loss\"],\n",
    "            \"manual_results\": manual_results,\n",
    "        }\n",
    "        print(f\"\\nüèÜ Best Manual Configuration:\")\n",
    "        print(f\"   ‚Ä¢ Validation Loss: {best_manual['val_loss']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1eb7e7",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Analysis & Visualization\n",
    "\n",
    "Analyzing optimization results and visualizing performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3640e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# üìä Step 8: Results Analysis and Visualization\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìä Analyzing Optimization Results...\")\n",
    "\n",
    "# Helper function to convert numpy types for JSON serialization\n",
    "\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# Check if optimization was completed successfully\n",
    "if optimization_results and \"analysis\" in optimization_results:\n",
    "    analysis = optimization_results[\"analysis\"]\n",
    "\n",
    "    # Get best configuration and results\n",
    "    best_result = analysis.get_best_trial(metric=\"val_loss\", mode=\"min\")\n",
    "    best_config = best_result.config\n",
    "    best_val_loss = best_result.last_result[\"val_loss\"]\n",
    "\n",
    "    print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "    print(f\"   ‚Ä¢ Validation Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ CNN Filters: {best_config['cnn_filters']}\")\n",
    "    print(f\"   ‚Ä¢ LSTM Units: {best_config['lstm_units']}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {best_config['learning_rate']:.2e}\")\n",
    "    print(f\"   ‚Ä¢ Dropout Rate: {best_config['dropout_rate']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Batch Size: {best_config['batch_size']}\")\n",
    "\n",
    "    # Visualization of hyperparameter optimization results\n",
    "    print(f\"\\nüìà Creating optimization analysis plots...\")\n",
    "\n",
    "    # Extract data for plotting\n",
    "    val_losses = [\n",
    "        trial.last_result[\"val_loss\"] for trial in analysis.trials if trial.last_result\n",
    "    ]\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\n",
    "        \"CNN-LSTM Hyperparameter Optimization Results\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # 1. CNN filters vs performance\n",
    "    cnn_filters = [\n",
    "        trial.config[\"cnn_filters\"] for trial in analysis.trials if trial.last_result\n",
    "    ]\n",
    "    axes[0, 0].scatter(cnn_filters, val_losses, alpha=0.7, color=\"blue\", s=60)\n",
    "    axes[0, 0].set_xlabel(\"CNN Filters\")\n",
    "    axes[0, 0].set_ylabel(\"Validation Loss\")\n",
    "    axes[0, 0].set_title(\"CNN Filters vs Performance\")\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. LSTM units vs performance\n",
    "    lstm_units = [\n",
    "        trial.config[\"lstm_units\"] for trial in analysis.trials if trial.last_result\n",
    "    ]\n",
    "    axes[0, 1].scatter(lstm_units, val_losses, alpha=0.7, color=\"green\", s=60)\n",
    "    axes[0, 1].set_xlabel(\"LSTM Units\")\n",
    "    axes[0, 1].set_ylabel(\"Validation Loss\")\n",
    "    axes[0, 1].set_title(\"LSTM Units vs Performance\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Learning rate vs performance\n",
    "    learning_rates = [\n",
    "        trial.config[\"learning_rate\"] for trial in analysis.trials if trial.last_result\n",
    "    ]\n",
    "    axes[1, 0].scatter(learning_rates, val_losses, alpha=0.7, color=\"orange\", s=60)\n",
    "    axes[1, 0].set_xscale(\"log\")\n",
    "    axes[1, 0].set_xlabel(\"Learning Rate (log scale)\")\n",
    "    axes[1, 0].set_ylabel(\"Validation Loss\")\n",
    "    axes[1, 0].set_title(\"Learning Rate vs Performance\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Optimization progress\n",
    "    trial_numbers = list(range(1, len(val_losses) + 1))\n",
    "    axes[1, 1].plot(\n",
    "        trial_numbers, val_losses, \"o-\", color=\"purple\", markersize=6, linewidth=2\n",
    "    )\n",
    "    axes[1, 1].axhline(\n",
    "        best_val_loss,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Best: {best_val_loss:.6f}\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Trial Number\")\n",
    "    axes[1, 1].set_ylabel(\"Validation Loss\")\n",
    "    axes[1, 1].set_title(\"Optimization Progress\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Performance statistics\n",
    "    print(f\"\\nüìà Performance Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Mean validation loss: {np.mean(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std validation loss: {np.std(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Min validation loss: {np.min(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Max validation loss: {np.max(val_losses):.6f}\")\n",
    "\n",
    "    # Save results to disk\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = \"optimization_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    best_config_serializable = convert_numpy_types(best_config)\n",
    "\n",
    "    # Save best configuration\n",
    "    best_config_file = f\"{results_dir}/best_cnn_lstm_config_{timestamp}.json\"\n",
    "    with open(best_config_file, \"w\") as f:\n",
    "        json.dump(best_config_serializable, f, indent=2)\n",
    "\n",
    "    print(f\"\\nüíæ Results Saved:\")\n",
    "    print(f\"   ‚Ä¢ Best configuration: {best_config_file}\")\n",
    "\n",
    "    # Save detailed results\n",
    "    results_summary = {\n",
    "        \"optimization_method\": \"Ray Tune\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"best_config\": best_config_serializable,\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"total_trials\": len(analysis.trials),\n",
    "        \"successful_trials\": len(\n",
    "            [t for t in analysis.trials if t.status == \"TERMINATED\"]\n",
    "        ),\n",
    "        \"performance_stats\": {\n",
    "            \"mean_val_loss\": float(np.mean(val_losses)),\n",
    "            \"std_val_loss\": float(np.std(val_losses)),\n",
    "            \"min_val_loss\": float(np.min(val_losses)),\n",
    "            \"max_val_loss\": float(np.max(val_losses)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    results_file = f\"{results_dir}/cnn_lstm_hparam_results_{timestamp}.json\"\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Detailed results: {results_file}\")\n",
    "\n",
    "    # Save trial data as CSV for further analysis\n",
    "    trial_data = []\n",
    "    for i, trial in enumerate(analysis.trials):\n",
    "        if trial.last_result:\n",
    "            config_dict = convert_numpy_types(trial.config)\n",
    "            if not isinstance(config_dict, dict):\n",
    "                config_dict = {}\n",
    "            trial_info = {\n",
    "                \"trial_id\": i + 1,\n",
    "                \"val_loss\": trial.last_result[\"val_loss\"],\n",
    "                **config_dict,\n",
    "            }\n",
    "            trial_data.append(trial_info)\n",
    "\n",
    "    df_results = pd.DataFrame(trial_data)\n",
    "    csv_file = f\"{results_dir}/cnn_lstm_trials_{timestamp}.csv\"\n",
    "    df_results.to_csv(csv_file, index=False)\n",
    "    print(f\"   ‚Ä¢ Trial data (CSV): {csv_file}\")\n",
    "\n",
    "    print(f\"\\nüéâ Hyperparameter optimization analysis completed successfully!\")\n",
    "    print(f\"üèÜ Best configuration achieved validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    analysis_completed = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping results analysis - optimization not completed\")\n",
    "    print(\"   Run the Ray Tune optimization cell first to generate results.\")\n",
    "    analysis_completed = False\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"   1. Use the best configuration for final model training\")\n",
    "print(f\"   2. Evaluate on test data\")\n",
    "print(f\"   3. Consider ensembling multiple top configurations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
