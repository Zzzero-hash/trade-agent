{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48577999",
   "metadata": {},
   "source": [
    "# CNN-LSTM Hyperparameter Optimization\n",
    "**Phase 2.5 - Trading RL Agent Development**\n",
    "\n",
    "This notebook implements distributed hyperparameter optimization for the CNN-LSTM model used in financial time series prediction.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Test CNN-LSTM model architecture with sample trading data\n",
    "- Implement comprehensive hyperparameter search space  \n",
    "- Execute distributed optimization using Ray Tune\n",
    "- Analyze results and identify optimal configurations\n",
    "\n",
    "## üìã Progress Tracker\n",
    "- [ ] **Step 1**: Environment Setup & Data Loading\n",
    "- [ ] **Step 2**: Model Architecture Validation\n",
    "- [ ] **Step 3**: Training Pipeline Implementation\n",
    "- [ ] **Step 4**: Hyperparameter Search Configuration\n",
    "- [ ] **Step 5**: Ray Tune Integration & Execution\n",
    "- [ ] **Step 6**: Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c006f",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Environment Setup & Data Loading\n",
    "\n",
    "Setting up the Python environment and loading sample trading data for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1353c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment Configuration:\n",
      "   ‚Ä¢ Project root: /workspaces/trading-rl-agent\n",
      "   ‚Ä¢ Python version: 3.10.12\n",
      "   ‚Ä¢ PyTorch version: 2.3.1+cu121\n",
      "   ‚Ä¢ CUDA available: True\n",
      "   ‚Ä¢ GPU count: 1\n",
      "   ‚Ä¢ GPU 0: NVIDIA GeForce RTX 3050\n",
      "   ‚Ä¢ CPU count: 16\n",
      "   ‚Ä¢ Timestamp: 2025-06-15 04:40:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 04:40:58,360\tINFO worker.py:1631 -- Connecting to existing Ray cluster at address: 172.17.0.2:6379...\n",
      "2025-06-15 04:40:58,372\tINFO worker.py:1816 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project modules imported successfully\n",
      "\n",
      "üöÄ Ray Cluster Initialization:\n",
      "   ‚Ä¢ Connected to Ray cluster\n",
      "   ‚Ä¢ Available CPUs: 16.0\n",
      "   ‚Ä¢ Available GPUs: 1.0\n",
      "   ‚Ä¢ Optimal GPU config: {'use_gpu': True, 'recommended_batch_size': 32, 'mixed_precision': False, 'gpu_index': 0, 'gpu_name': 'NVIDIA GeForce RTX 3050', 'gpu_memory_free_mb': 5930.3046875, 'estimated_memory_required_mb': 1.67236328125, 'reason': 'Using full precision, memory is sufficient'}\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Environment Setup with Ray Cluster Integration\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"üîß Environment Configuration:\")\n",
    "print(f\"   ‚Ä¢ Project root: {project_root}\")\n",
    "print(f\"   ‚Ä¢ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"   ‚Ä¢ CPU count: {os.cpu_count()}\")\n",
    "print(f\"   ‚Ä¢ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Import project modules with error handling\n",
    "try:\n",
    "    from src.models.cnn_lstm import CNNLSTMModel, CNNLSTMConfig, create_model\n",
    "    from src.data_pipeline import PipelineConfig, load_data, generate_features, split_by_date\n",
    "    from src.utils.cluster import init_ray, get_available_devices\n",
    "    from src.optimization.model_summary import detect_gpus, optimal_gpu_config\n",
    "    print(\"‚úÖ Project modules imported successfully\")\n",
    "    modules_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Ensure you're running from the project root directory\")\n",
    "    modules_available = False\n",
    "\n",
    "# Initialize Ray cluster for distributed training\n",
    "print(\"\\nüöÄ Ray Cluster Initialization:\")\n",
    "try:\n",
    "    # Check if Ray is already initialized\n",
    "    import ray\n",
    "    if ray.is_initialized():\n",
    "        print(\"   ‚Ä¢ Ray already initialized\")\n",
    "        ray_ready = True\n",
    "    else:\n",
    "        # Try to connect to existing cluster first, then local\n",
    "        try:\n",
    "            init_ray(local_mode=False)  # Try to connect to cluster\n",
    "            print(\"   ‚Ä¢ Connected to Ray cluster\")\n",
    "            ray_ready = True\n",
    "        except Exception:\n",
    "            init_ray(local_mode=True)  # Fallback to local mode\n",
    "            print(\"   ‚Ä¢ Ray initialized in local mode\")\n",
    "            ray_ready = True\n",
    "    \n",
    "    # Get available resources\n",
    "    if ray_ready:\n",
    "        resources = get_available_devices()\n",
    "        print(f\"   ‚Ä¢ Available CPUs: {resources.get('CPU', 0)}\")\n",
    "        print(f\"   ‚Ä¢ Available GPUs: {resources.get('GPU', 0)}\")\n",
    "        \n",
    "        # Get optimal GPU configuration\n",
    "        gpu_config = optimal_gpu_config(\n",
    "            model_params=100000,  # Approximate parameter count for CNN-LSTM\n",
    "            batch_size=32,\n",
    "            sequence_length=60,\n",
    "            feature_dim=10\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ Optimal GPU config: {gpu_config}\")\n",
    "        \n",
    "        ray_available = True\n",
    "        \n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Ray initialization failed\")\n",
    "        ray_available = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Ray initialization failed: {e}\")\n",
    "    print(\"   ‚Ä¢ Continuing without distributed training\")\n",
    "    ray_ready = False\n",
    "    ray_available = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35142e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Sample Trading Data...\n",
      "   ‚Ä¢ Available data files: 2\n",
      "‚úÖ Data loaded successfully: data/sample_training_data_simple_20250607_192034.csv\n",
      "   ‚Ä¢ Shape: (3827, 29)\n",
      "   ‚Ä¢ Columns: ['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'sma_10', 'sma_20', 'sma_50', 'ema_12', 'ema_26', 'macd', 'macd_signal', 'rsi', 'bb_middle', 'bb_upper', 'bb_lower', 'bb_position', 'volume_sma', 'volume_ratio', 'price_change', 'price_change_5', 'volatility', 'news_sentiment', 'social_sentiment', 'composite_sentiment', 'sentiment_volume', 'label']\n",
      "   ‚Ä¢ Date range: 2024-12-11 20:20:34.593017 to 2025-05-20 06:20:34.593017\n",
      "\n",
      "üìã Sample Data Preview:\n",
      "                    timestamp symbol        open        high         low  \\\n",
      "0  2024-12-11 20:20:34.593017   AAPL  147.146236  147.146236  140.637088   \n",
      "1  2024-12-11 21:20:34.593017   AAPL  142.342309  146.096217  142.342309   \n",
      "2  2024-12-11 22:20:34.593017   AAPL  145.166743  146.159014  145.023960   \n",
      "\n",
      "        close   volume      sma_10      sma_20      sma_50  ...  volume_sma  \\\n",
      "0  142.420076  1096798  146.964050  151.445764  156.360177  ...  1121689.15   \n",
      "1  145.325456   629532  146.245458  151.314223  156.229201  ...  1082182.70   \n",
      "2  145.984476  1028941  145.569575  150.742576  156.113333  ...  1077838.30   \n",
      "\n",
      "   volume_ratio  price_change  price_change_5  volatility  news_sentiment  \\\n",
      "0      0.977809     -0.033805       -0.015116    0.026780       -0.522588   \n",
      "1      0.581724      0.020400        0.023245    0.027225        0.083703   \n",
      "2      0.954634      0.004535        0.037637    0.022679       -0.107365   \n",
      "\n",
      "   social_sentiment  composite_sentiment  sentiment_volume  label  \n",
      "0         -0.871111            -0.661997                 8      2  \n",
      "1         -0.273021            -0.058986                13      1  \n",
      "2         -0.546061            -0.282843                12      1  \n",
      "\n",
      "[3 rows x 29 columns]\n",
      "\n",
      "üßπ Cleaning sample data...\n",
      "\n",
      "üî¢ Feature Data (After Cleaning):\n",
      "   ‚Ä¢ Features: ['open', 'high', 'low', 'close', 'volume']\n",
      "   ‚Ä¢ Shape: (3827, 5)\n",
      "   ‚Ä¢ Data type: float32\n",
      "   ‚Ä¢ NaN values: 0\n",
      "   ‚Ä¢ Infinite values: 7994\n",
      "‚ö†Ô∏è  Remaining data quality issues: 0 NaN, 7994 infinite values\n",
      "   ‚Ä¢ Applied final cleanup with np.nan_to_num\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Loading & Preparation\n",
    "print(\"üìä Loading Sample Trading Data...\")\n",
    "\n",
    "# Load available sample data\n",
    "data_files = [f for f in os.listdir('data/') if f.startswith('sample_training_data') and f.endswith('.csv')]\n",
    "print(f\"   ‚Ä¢ Available data files: {len(data_files)}\")\n",
    "\n",
    "if data_files:\n",
    "    # Use the simple sample data for testing\n",
    "    sample_file = 'data/sample_training_data_simple_20250607_192034.csv'\n",
    "    \n",
    "    try:\n",
    "        df_sample = pd.read_csv(sample_file)\n",
    "        print(f\"‚úÖ Data loaded successfully: {sample_file}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {df_sample.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(df_sample.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Date range: {df_sample['timestamp'].iloc[0]} to {df_sample['timestamp'].iloc[-1]}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nüìã Sample Data Preview:\")\n",
    "        print(df_sample.head(3))\n",
    "        \n",
    "        # Prepare feature data (OHLCV) with data cleaning\n",
    "        feature_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "        # Clean the data before processing\n",
    "        print(\"\\nüßπ Cleaning sample data...\")\n",
    "        \n",
    "        # Replace infinite values with NaN first\n",
    "        df_clean = df_sample[feature_columns].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Fill NaN values with reasonable estimates\n",
    "        for col in feature_columns:\n",
    "            if col == 'volume':\n",
    "                # Use median for volume\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            else:\n",
    "                # Use forward fill then backward fill for prices\n",
    "                df_clean[col] = df_clean[col].fillna(method='ffill').fillna(method='bfill')\n",
    "                # If still NaN (entire column), use a reasonable default\n",
    "                if df_clean[col].isna().all():\n",
    "                    df_clean[col] = 100.0  # Default price\n",
    "        \n",
    "        # Convert to float32 for numerical stability\n",
    "        X_raw = df_clean.values.astype(np.float32)\n",
    "        \n",
    "        print(f\"\\nüî¢ Feature Data (After Cleaning):\")\n",
    "        print(f\"   ‚Ä¢ Features: {feature_columns}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {X_raw.shape}\")\n",
    "        print(f\"   ‚Ä¢ Data type: {X_raw.dtype}\")\n",
    "        \n",
    "        # Check for any remaining data quality issues\n",
    "        nan_count = np.isnan(X_raw).sum()\n",
    "        inf_count = np.isinf(X_raw).sum()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ NaN values: {nan_count}\")\n",
    "        print(f\"   ‚Ä¢ Infinite values: {inf_count}\")\n",
    "        \n",
    "        if nan_count == 0 and inf_count == 0:\n",
    "            print(\"‚úÖ Data quality checks passed\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Remaining data quality issues: {nan_count} NaN, {inf_count} infinite values\")\n",
    "            # Final cleanup if any issues remain\n",
    "            X_raw = np.nan_to_num(X_raw, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "            print(\"   ‚Ä¢ Applied final cleanup with np.nan_to_num\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        df_sample = None\n",
    "        X_raw = None\n",
    "else:\n",
    "    print(\"‚ùå No sample data files found\")\n",
    "    print(\"   Run generate_sample_data.py to create sample data\")\n",
    "    df_sample = None\n",
    "    X_raw = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f779f2",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Model Architecture Validation\n",
    "\n",
    "Testing the CNN-LSTM model architecture with our sample data to ensure compatibility before hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bdd898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating and Testing Optimized CNN-LSTM Model...\n",
      "   ‚Ä¢ Adaptive sequence length: 30\n",
      "   ‚Ä¢ Feature dimensions: 5\n",
      "   ‚Ä¢ Total samples available: 3827\n",
      "   ‚Ä¢ Sequence data shape: X=(3797, 30, 5), y=(3797,)\n",
      "\n",
      "üîß Enhanced Model Configuration:\n",
      "   ‚Ä¢ Input dimensions: 5\n",
      "   ‚Ä¢ CNN filters: [32, 64]\n",
      "   ‚Ä¢ CNN kernels: [3, 5]\n",
      "   ‚Ä¢ LSTM units: 50\n",
      "   ‚Ä¢ Dropout: 0.2\n",
      "   ‚Ä¢ Using GPU: NVIDIA GeForce RTX 3050\n",
      "\n",
      "‚úÖ Model created successfully on cuda:0\n",
      "   ‚Ä¢ Total parameters: 34,067\n",
      "   ‚Ä¢ Trainable parameters: 34,067\n",
      "   ‚Ä¢ Model memory usage: 0.1 MB\n",
      "   ‚Ä¢ Batch size 4: ‚úÖ torch.Size([4, 30, 5]) ‚Üí torch.Size([4, 1])\n",
      "   ‚Ä¢ Sample predictions: [ 0.27331877  0.03934449 -0.20274957]\n",
      "   ‚Ä¢ Batch size 8: ‚úÖ torch.Size([8, 30, 5]) ‚Üí torch.Size([8, 1])\n",
      "   ‚Ä¢ Batch size 16: ‚úÖ torch.Size([16, 30, 5]) ‚Üí torch.Size([16, 1])\n",
      "\n",
      "üìä Validation Summary:\n",
      "   ‚Ä¢ Architecture: ‚úÖ Validated\n",
      "   ‚Ä¢ Device compatibility: ‚úÖ cuda:0\n",
      "   ‚Ä¢ Memory efficiency: ‚úÖ Optimized\n",
      "   ‚Ä¢ Ready for hyperparameter optimization\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture Validation\n",
    "if X_raw is not None and modules_available:\n",
    "    print(\"üèóÔ∏è Creating and Testing Optimized CNN-LSTM Model...\")\n",
    "    \n",
    "    # Prepare sequence data for time series prediction\n",
    "    sequence_length = min(30, len(X_raw) - 10)  # Adaptive sequence length\n",
    "    step_size = 1\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Adaptive sequence length: {sequence_length}\")\n",
    "    print(f\"   ‚Ä¢ Feature dimensions: {X_raw.shape[1]}\")\n",
    "    print(f\"   ‚Ä¢ Total samples available: {len(X_raw)}\")\n",
    "    \n",
    "    # Create sequences with validation\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(len(X_raw) - sequence_length):\n",
    "        X_sequences.append(X_raw[i:i + sequence_length])\n",
    "        # Predict next close price (index 3 in OHLCV)\n",
    "        y_sequences.append(X_raw[i + sequence_length, 3])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Sequence data shape: X={X_sequences.shape}, y={y_sequences.shape}\")\n",
    "    \n",
    "    # Enhanced model configuration with optimal settings\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],  # Number of features (5 for OHLCV)\n",
    "        output_size=1,                    # Single prediction output\n",
    "        cnn_filters=[32, 64],            # CNN layer sizes\n",
    "        cnn_kernel_sizes=[3, 5],         # Different kernel sizes for pattern detection\n",
    "        lstm_units=50,                   # LSTM hidden units\n",
    "        dropout=0.2,                     # Dropout rate\n",
    "        use_attention=False              # No attention for baseline\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîß Enhanced Model Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Input dimensions: {test_config.input_dim}\")\n",
    "    print(f\"   ‚Ä¢ CNN filters: {test_config.cnn_filters}\")\n",
    "    print(f\"   ‚Ä¢ CNN kernels: {test_config.cnn_kernel_sizes}\")\n",
    "    print(f\"   ‚Ä¢ LSTM units: {test_config.lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Dropout: {test_config.dropout}\")\n",
    "    \n",
    "    # Device selection with optimization\n",
    "    if torch.cuda.is_available() and resources.get('GPU', 0) > 0:\n",
    "        device = torch.device('cuda:0')  # Use first GPU\n",
    "        print(f\"   ‚Ä¢ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(f\"   ‚Ä¢ Using CPU with {resources.get('CPU', 1)} cores\")\n",
    "    \n",
    "    # Create and test model with memory optimization\n",
    "    try:\n",
    "        model = create_model(test_config)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model created successfully on {device}\")\n",
    "        print(f\"   ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "        print(f\"   ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Memory usage estimation\n",
    "        if device.type == 'cuda':\n",
    "            model_memory_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "            print(f\"   ‚Ä¢ Model memory usage: {model_memory_mb:.1f} MB\")\n",
    "        \n",
    "        # Test forward pass with different batch sizes\n",
    "        test_batch_sizes = [4, 8, 16] if len(X_sequences) >= 16 else [min(4, len(X_sequences))]\n",
    "        \n",
    "        for batch_size in test_batch_sizes:\n",
    "            if batch_size <= len(X_sequences):\n",
    "                try:\n",
    "                    X_test = torch.FloatTensor(X_sequences[:batch_size]).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        test_output = model(X_test)\n",
    "                    \n",
    "                    print(f\"   ‚Ä¢ Batch size {batch_size}: ‚úÖ {X_test.shape} ‚Üí {test_output.shape}\")\n",
    "                    \n",
    "                    if batch_size == test_batch_sizes[0]:  # Show predictions for first batch\n",
    "                        sample_preds = test_output.flatten()[:3].cpu().numpy()\n",
    "                        print(f\"   ‚Ä¢ Sample predictions: {sample_preds}\")\n",
    "                    \n",
    "                    # Clear cache for next test\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"   ‚Ä¢ Batch size {batch_size}: ‚ùå {str(e)}\")\n",
    "        \n",
    "        architecture_validated = True\n",
    "        \n",
    "        # Store configuration for hyperparameter optimization\n",
    "        validated_config = {\n",
    "            'sequence_length': sequence_length,\n",
    "            'input_dim': test_config.input_dim,\n",
    "            'data_shape': X_sequences.shape,\n",
    "            'model_params': total_params,\n",
    "            'device': str(device)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Validation Summary:\")\n",
    "        print(f\"   ‚Ä¢ Architecture: ‚úÖ Validated\")\n",
    "        print(f\"   ‚Ä¢ Device compatibility: ‚úÖ {device}\")\n",
    "        print(f\"   ‚Ä¢ Memory efficiency: ‚úÖ Optimized\")\n",
    "        print(f\"   ‚Ä¢ Ready for hyperparameter optimization\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model architecture error: {e}\")\n",
    "        architecture_validated = False\n",
    "        validated_config = None\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model validation - dependencies not available\")\n",
    "    architecture_validated = False\n",
    "    validated_config = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7cb1f",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Comprehensive Training Pipeline\n",
    "\n",
    "Implementing a robust training pipeline with loss calculation, metrics tracking, and distributed optimization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tune Compatible Training Function\n",
    "def train_cnn_lstm_ray(config, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Ray Tune compatible training function for CNN-LSTM hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        config: Hyperparameter configuration from Ray Tune\n",
    "        checkpoint_dir: Directory for saving/loading checkpoints\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from ray import tune, train  # Import both tune and train for Ray 2.0+\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    learning_rate = config.get('learning_rate', 0.001)\n",
    "    batch_size = config.get('batch_size', 32)\n",
    "    num_epochs = config.get('num_epochs', 50)\n",
    "    cnn_filters = config.get('cnn_filters', [32, 64])\n",
    "    lstm_units = config.get('lstm_units', 50)\n",
    "    dropout = config.get('dropout', 0.2)\n",
    "    cnn_kernel_sizes = config.get('cnn_kernel_sizes', [3, 5])\n",
    "    \n",
    "    # Global data (will be passed from outer scope)\n",
    "    global X_sequences_train, y_sequences_train, X_sequences_val, y_sequences_val\n",
    "    \n",
    "    # Device selection\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model configuration\n",
    "    model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences_train.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=cnn_filters,\n",
    "        cnn_kernel_sizes=cnn_kernel_sizes,\n",
    "        lstm_units=lstm_units,\n",
    "        dropout=dropout,\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(model_config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_train),\n",
    "        torch.FloatTensor(y_sequences_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_val),\n",
    "        torch.FloatTensor(y_sequences_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_dir:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            train_samples += data.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / train_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output.squeeze(), target)\n",
    "                \n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                val_samples += data.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if checkpoint_dir and (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        \n",
    "        # Track best validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "        \n",
    "        # Report metrics to Ray Tune (Updated for Ray 2.0+)\n",
    "        train.report({\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1451711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Preparing Data for Distributed Training...\n",
      "üßπ Cleaning data for training...\n",
      "   ‚Ä¢ Original samples: 3797\n",
      "   ‚Ä¢ Samples with NaN: 0\n",
      "   ‚Ä¢ Samples with infinity: 0\n",
      "   ‚Ä¢ Samples with invalid targets: 0\n",
      "   ‚Ä¢ Total invalid samples: 0\n",
      "   ‚Ä¢ Valid samples remaining: 3797\n",
      "   ‚Ä¢ Data cleaned successfully: 3797/3797 samples retained\n",
      "   ‚Ä¢ Applied extreme value clipping for numerical stability\n",
      "   ‚Ä¢ Training samples: 3037\n",
      "   ‚Ä¢ Validation samples: 760\n",
      "   ‚Ä¢ Train/Val split: 80.0%/20.0%\n",
      "   ‚Ä¢ Feature scaling: ‚úÖ Applied StandardScaler\n",
      "   ‚Ä¢ Target scaling: ‚úÖ Applied StandardScaler\n",
      "\n",
      "üîç Final data validation:\n",
      "   ‚Ä¢ X_train shape: (3037, 30, 5)\n",
      "   ‚Ä¢ y_train shape: (3037,)\n",
      "   ‚Ä¢ X_val shape: (760, 30, 5)\n",
      "   ‚Ä¢ y_val shape: (760,)\n",
      "   ‚Ä¢ X_train NaN/inf: 0\n",
      "   ‚Ä¢ y_train NaN/inf: 0\n",
      "   ‚Ä¢ X_val NaN/inf: 0\n",
      "   ‚Ä¢ y_val NaN/inf: 0\n",
      "   ‚Ä¢ Data retention rate: 100.0%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation for Distributed Training\n",
    "if architecture_validated and X_sequences is not None:\n",
    "    print(\"üìä Preparing Data for Distributed Training...\")\n",
    "    \n",
    "    # Data Quality Check and Cleaning\n",
    "    print(\"üßπ Cleaning data for training...\")\n",
    "    \n",
    "    # Check for data quality issues in sequences\n",
    "    original_samples = len(X_sequences)\n",
    "    print(f\"   ‚Ä¢ Original samples: {original_samples}\")\n",
    "    \n",
    "    # Convert to numpy arrays for easier processing\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "    \n",
    "    # Check for NaN and infinite values\n",
    "    nan_mask = np.isnan(X_sequences).any(axis=(1, 2))  # Any NaN in sequence\n",
    "    inf_mask = np.isinf(X_sequences).any(axis=(1, 2))  # Any inf in sequence\n",
    "    y_nan_mask = np.isnan(y_sequences) | np.isinf(y_sequences)  # NaN/inf in targets\n",
    "    \n",
    "    # Combine all invalid data masks\n",
    "    invalid_mask = nan_mask | inf_mask | y_nan_mask\n",
    "    valid_mask = ~invalid_mask\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Samples with NaN: {nan_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Samples with infinity: {inf_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Samples with invalid targets: {y_nan_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Total invalid samples: {invalid_mask.sum()}\")\n",
    "    \n",
    "    # Remove invalid samples\n",
    "    if valid_mask.sum() == 0:\n",
    "        raise ValueError(\"No valid data remaining after cleaning. Please check data generation process.\")\n",
    "    \n",
    "    X_sequences = X_sequences[valid_mask]\n",
    "    y_sequences = y_sequences[valid_mask]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Valid samples remaining: {len(X_sequences)}\")\n",
    "    print(f\"   ‚Ä¢ Data cleaned successfully: {len(X_sequences)}/{original_samples} samples retained\")\n",
    "    \n",
    "    # Additional safety: clip extreme values that might still cause issues\n",
    "    # Use reasonable bounds for financial data\n",
    "    X_sequences = np.clip(X_sequences, -1e6, 1e6)  # Clip extreme values\n",
    "    y_sequences = np.clip(y_sequences, -1e6, 1e6)\n",
    "    \n",
    "    print(\"   ‚Ä¢ Applied extreme value clipping for numerical stability\")\n",
    "    \n",
    "    # Split data for training and validation\n",
    "    total_samples = len(X_sequences)\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    val_size = total_samples - train_size\n",
    "    \n",
    "    # Time-based split to avoid data leakage\n",
    "    X_sequences_train = X_sequences[:train_size]\n",
    "    y_sequences_train = y_sequences[:train_size]\n",
    "    X_sequences_val = X_sequences[train_size:]\n",
    "    y_sequences_val = y_sequences[train_size:]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Training samples: {len(X_sequences_train)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {len(X_sequences_val)}\")\n",
    "    print(f\"   ‚Ä¢ Train/Val split: {train_size/total_samples:.1%}/{val_size/total_samples:.1%}\")\n",
    "    \n",
    "    # Data normalization for better training stability\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Reshape for scaler (samples * timesteps, features)\n",
    "    X_train_reshaped = X_sequences_train.reshape(-1, X_sequences_train.shape[-1])\n",
    "    \n",
    "    # Final safety check before scaling\n",
    "    if np.isnan(X_train_reshaped).any() or np.isinf(X_train_reshaped).any():\n",
    "        print(\"‚ö†Ô∏è  Warning: Still found NaN/inf values after cleaning. Applying final cleanup...\")\n",
    "        # Replace any remaining NaN/inf with median values\n",
    "        X_train_reshaped = np.nan_to_num(X_train_reshaped, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
    "    X_sequences_train = X_train_scaled.reshape(X_sequences_train.shape)\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val_reshaped = X_sequences_val.reshape(-1, X_sequences_val.shape[-1])\n",
    "    if np.isnan(X_val_reshaped).any() or np.isinf(X_val_reshaped).any():\n",
    "        X_val_reshaped = np.nan_to_num(X_val_reshaped, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    X_val_scaled = scaler_X.transform(X_val_reshaped)\n",
    "    X_sequences_val = X_val_scaled.reshape(X_sequences_val.shape)\n",
    "    \n",
    "    # Scale targets\n",
    "    y_sequences_train_clean = np.nan_to_num(y_sequences_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_sequences_val_clean = np.nan_to_num(y_sequences_val, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    y_sequences_train = scaler_y.fit_transform(y_sequences_train_clean.reshape(-1, 1)).flatten()\n",
    "    y_sequences_val = scaler_y.transform(y_sequences_val_clean.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Feature scaling: ‚úÖ Applied StandardScaler\")\n",
    "    print(f\"   ‚Ä¢ Target scaling: ‚úÖ Applied StandardScaler\")\n",
    "    \n",
    "    # Final validation check\n",
    "    print(\"\\nüîç Final data validation:\")\n",
    "    print(f\"   ‚Ä¢ X_train shape: {X_sequences_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ y_train shape: {y_sequences_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ X_val shape: {X_sequences_val.shape}\")\n",
    "    print(f\"   ‚Ä¢ y_val shape: {y_sequences_val.shape}\")\n",
    "    print(f\"   ‚Ä¢ X_train NaN/inf: {np.isnan(X_sequences_train).sum() + np.isinf(X_sequences_train).sum()}\")\n",
    "    print(f\"   ‚Ä¢ y_train NaN/inf: {np.isnan(y_sequences_train).sum() + np.isinf(y_sequences_train).sum()}\")\n",
    "    print(f\"   ‚Ä¢ X_val NaN/inf: {np.isnan(X_sequences_val).sum() + np.isinf(X_sequences_val).sum()}\")\n",
    "    print(f\"   ‚Ä¢ y_val NaN/inf: {np.isnan(y_sequences_val).sum() + np.isinf(y_sequences_val).sum()}\")\n",
    "    \n",
    "    data_prepared = True\n",
    "    \n",
    "    # Store scalers for later use\n",
    "    preprocessing_artifacts = {\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'feature_columns': ['open', 'high', 'low', 'close', 'volume'],\n",
    "        'sequence_length': sequence_length,\n",
    "        'original_samples': original_samples,\n",
    "        'cleaned_samples': len(X_sequences),\n",
    "        'cleaning_stats': {\n",
    "            'nan_samples': nan_mask.sum(),\n",
    "            'inf_samples': inf_mask.sum(),\n",
    "            'invalid_targets': y_nan_mask.sum(),\n",
    "            'retention_rate': len(X_sequences) / original_samples\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Data retention rate: {preprocessing_artifacts['cleaning_stats']['retention_rate']:.1%}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping data preparation - model validation failed\")\n",
    "    data_prepared = False\n",
    "    preprocessing_artifacts = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1481f",
   "metadata": {},
   "source": [
    "## üîç Step 4: Comprehensive Hyperparameter Search Space\n",
    "\n",
    "Defining an extensive search space for CNN-LSTM hyperparameter optimization with intelligent resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa07b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Configuring Comprehensive Hyperparameter Search Space...\n",
      "   ‚Ä¢ GPU mode: 1 concurrent trials\n",
      "   ‚Ä¢ Resources per trial: {'cpu': 16, 'gpu': 1.0}\n",
      "   ‚Ä¢ Search space size: 7 parameters\n",
      "   ‚Ä¢ Planned trials: 50\n",
      "   ‚Ä¢ Scheduler: ASHA with early stopping\n",
      "   ‚Ä¢ Search algorithm: Random search (default)\n",
      "   ‚Ä¢ Concurrent trials: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Hyperparameter Search Space Configuration\n",
    "if data_prepared and ray_ready:\n",
    "    print(\"üîç Configuring Comprehensive Hyperparameter Search Space...\")\n",
    "    \n",
    "    from ray import tune\n",
    "    from ray.tune.schedulers import ASHAScheduler\n",
    "    from ray.tune.stopper import TrialPlateauStopper\n",
    "    \n",
    "    # Define comprehensive search space\n",
    "    search_space = {\n",
    "        # Architecture parameters\n",
    "        'cnn_filters': tune.choice([\n",
    "            [16, 32],           # Lightweight\n",
    "            [32, 64],           # Baseline\n",
    "            [64, 128],          # Enhanced\n",
    "            [32, 64, 128],      # Deep\n",
    "        ]),\n",
    "        'cnn_kernel_sizes': tune.choice([\n",
    "            [3, 3],             # Same size kernels\n",
    "            [3, 5],             # Progressive kernels\n",
    "            [3, 5, 7],          # Multi-scale (for 3-layer CNN)\n",
    "            [5, 5],             # Larger kernels\n",
    "        ]),\n",
    "        'lstm_units': tune.choice([32, 50, 64, 100, 128]),\n",
    "        'dropout': tune.uniform(0.1, 0.4),\n",
    "        \n",
    "        # Training parameters\n",
    "        'learning_rate': tune.loguniform(1e-4, 1e-2),\n",
    "        'batch_size': tune.choice([16, 32, 64]),\n",
    "        'num_epochs': tune.choice([30, 50, 75]),\n",
    "    }\n",
    "    \n",
    "    # Advanced search space for extensive optimization\n",
    "    advanced_search_space = {\n",
    "        **search_space,\n",
    "        'weight_decay': tune.loguniform(1e-6, 1e-3),\n",
    "        'gradient_clip': tune.uniform(0.5, 2.0),\n",
    "        'optimizer_type': tune.choice(['adam', 'adamw', 'rmsprop']),\n",
    "        'lr_scheduler': tune.choice(['plateau', 'cosine', 'step']),\n",
    "    }\n",
    "    \n",
    "    # Resource allocation based on available hardware\n",
    "    if resources.get('GPU', 0) >= 1:\n",
    "        # GPU-optimized configuration\n",
    "        num_samples = 50  # More trials with GPUs\n",
    "        max_concurrent_trials = min(int(resources['GPU']), 4)\n",
    "        resources_per_trial = {\n",
    "            'cpu': max(1, int(resources['CPU'] / max_concurrent_trials)),\n",
    "            'gpu': resources['GPU'] / max_concurrent_trials\n",
    "        }\n",
    "        print(f\"   ‚Ä¢ GPU mode: {max_concurrent_trials} concurrent trials\")\n",
    "        print(f\"   ‚Ä¢ Resources per trial: {resources_per_trial}\")\n",
    "    else:\n",
    "        # CPU-optimized configuration\n",
    "        num_samples = 20  # Fewer trials for CPU\n",
    "        max_concurrent_trials = min(int(resources['CPU'] / 2), 4)\n",
    "        resources_per_trial = {\n",
    "            'cpu': max(1, int(resources['CPU'] / max_concurrent_trials))\n",
    "        }\n",
    "        print(f\"   ‚Ä¢ CPU mode: {max_concurrent_trials} concurrent trials\")\n",
    "        print(f\"   ‚Ä¢ Resources per trial: {resources_per_trial}\")\n",
    "    \n",
    "    # Advanced schedulers for efficient search\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=search_space['num_epochs'].categories[0] if hasattr(search_space['num_epochs'], 'categories') else 50,\n",
    "        grace_period=10,  # Minimum epochs before stopping\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    \n",
    "    # Use basic random search instead of OptunaSearch for simplicity\n",
    "    search_alg = None  # Use Ray Tune's default random search\n",
    "    \n",
    "    # Early stopping based on plateau\n",
    "    stopper = TrialPlateauStopper(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_results=10,  # Look at last 10 results\n",
    "        grace_period=20  # Minimum iterations before stopping\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Search space size: {len(search_space)} parameters\")\n",
    "    print(f\"   ‚Ä¢ Planned trials: {num_samples}\")\n",
    "    print(f\"   ‚Ä¢ Scheduler: ASHA with early stopping\")\n",
    "    print(f\"   ‚Ä¢ Search algorithm: Random search (default)\")\n",
    "    print(f\"   ‚Ä¢ Concurrent trials: {max_concurrent_trials}\")\n",
    "    \n",
    "    # Create experiment configuration\n",
    "    experiment_config = {\n",
    "        'search_space': search_space,\n",
    "        'advanced_search_space': advanced_search_space,\n",
    "        'num_samples': num_samples,\n",
    "        'max_concurrent_trials': max_concurrent_trials,\n",
    "        'resources_per_trial': resources_per_trial,\n",
    "        'scheduler': scheduler,\n",
    "        'search_alg': search_alg,\n",
    "        'stopper': stopper\n",
    "    }\n",
    "    \n",
    "    search_configured = True\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping search space configuration - prerequisites not met\")\n",
    "    search_configured = False\n",
    "    experiment_config = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ecb0",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Distributed Ray Tune Execution\n",
    "\n",
    "Executing the hyperparameter optimization with full resource utilization and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c6f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-15 04:43:46</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:46.92        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.2/23.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 20.000: None | Iter 10.000: None<br>Logical resource usage: 16.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 9 more trials not shown (9 ERROR)\n",
       "  Number of errored trials: 28<br>Table truncated to 20 rows (8 overflow)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00000_0_batch_size=64,cnn_filters=32_64_128,cnn_kernel_sizes=3_5,dropout=0.2154,learning_rate=0.0015,lstm_2025-06-15_04-40-59/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00001_1_batch_size=32,cnn_filters=32_64_128,cnn_kernel_sizes=3_5_7,dropout=0.2834,learning_rate=0.0059,ls_2025-06-15_04-41-04/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00002_2_batch_size=64,cnn_filters=32_64,cnn_kernel_sizes=5_5,dropout=0.3447,learning_rate=0.0002,lstm_uni_2025-06-15_04-41-10/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00003</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00003_3_batch_size=16,cnn_filters=32_64,cnn_kernel_sizes=3_5_7,dropout=0.1250,learning_rate=0.0082,lstm_u_2025-06-15_04-41-16/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00004</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00004_4_batch_size=32,cnn_filters=64_128,cnn_kernel_sizes=3_3,dropout=0.1893,learning_rate=0.0063,lstm_un_2025-06-15_04-41-23/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00005</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00005_5_batch_size=64,cnn_filters=64_128,cnn_kernel_sizes=3_3,dropout=0.2357,learning_rate=0.0017,lstm_un_2025-06-15_04-41-28/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00006</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00006_6_batch_size=64,cnn_filters=32_64_128,cnn_kernel_sizes=3_5,dropout=0.3910,learning_rate=0.0001,lstm_2025-06-15_04-41-34/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00007</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00007_7_batch_size=32,cnn_filters=32_64,cnn_kernel_sizes=5_5,dropout=0.3535,learning_rate=0.0007,lstm_uni_2025-06-15_04-41-40/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00008</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00008_8_batch_size=16,cnn_filters=16_32,cnn_kernel_sizes=3_5,dropout=0.1300,learning_rate=0.0001,lstm_uni_2025-06-15_04-41-46/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00009</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00009_9_batch_size=16,cnn_filters=64_128,cnn_kernel_sizes=5_5,dropout=0.1400,learning_rate=0.0002,lstm_un_2025-06-15_04-41-53/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00010</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00010_10_batch_size=64,cnn_filters=32_64_128,cnn_kernel_sizes=3_5_7,dropout=0.3960,learning_rate=0.0011,l_2025-06-15_04-41-59/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00011</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00011_11_batch_size=64,cnn_filters=16_32,cnn_kernel_sizes=5_5,dropout=0.2865,learning_rate=0.0003,lstm_un_2025-06-15_04-42-04/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00012</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00012_12_batch_size=64,cnn_filters=64_128,cnn_kernel_sizes=3_5_7,dropout=0.3233,learning_rate=0.0004,lstm_2025-06-15_04-42-10/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00013</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00013_13_batch_size=32,cnn_filters=64_128,cnn_kernel_sizes=3_5,dropout=0.3413,learning_rate=0.0001,lstm_u_2025-06-15_04-42-15/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00014</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00014_14_batch_size=16,cnn_filters=32_64_128,cnn_kernel_sizes=5_5,dropout=0.3122,learning_rate=0.0063,lst_2025-06-15_04-42-20/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00015</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00015_15_batch_size=16,cnn_filters=32_64_128,cnn_kernel_sizes=5_5,dropout=0.3820,learning_rate=0.0015,lst_2025-06-15_04-42-27/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00016</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00016_16_batch_size=32,cnn_filters=32_64_128,cnn_kernel_sizes=3_3,dropout=0.1296,learning_rate=0.0008,lst_2025-06-15_04-42-33/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00017</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00017_17_batch_size=16,cnn_filters=32_64,cnn_kernel_sizes=5_5,dropout=0.2356,learning_rate=0.0004,lstm_un_2025-06-15_04-42-38/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00018</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00018_18_batch_size=64,cnn_filters=16_32,cnn_kernel_sizes=3_3,dropout=0.1281,learning_rate=0.0003,lstm_un_2025-06-15_04-42-45/error.txt</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00019</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-06-15_04-25-33_547174_84758/artifacts/2025-06-15_04-40-59/cnn_lstm_tune/driver_artifacts/train_cnn_lstm_ray_ef36c_00019_19_batch_size=16,cnn_filters=32_64_128,cnn_kernel_sizes=3_5,dropout=0.2706,learning_rate=0.0093,lst_2025-06-15_04-42-51/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  batch_size</th><th>cnn_filters  </th><th>cnn_kernel_sizes  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  lstm_units</th><th style=\"text-align: right;\">  num_epochs</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00028</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">          32</td><td>[32, 64]     </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.101183</td><td style=\"text-align: right;\">    0.00960787 </td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00000</td><td>ERROR   </td><td>172.17.0.2:91430</td><td style=\"text-align: right;\">          64</td><td>[32, 64, 128]</td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.215352</td><td style=\"text-align: right;\">    0.00154298 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00001</td><td>ERROR   </td><td>172.17.0.2:91583</td><td style=\"text-align: right;\">          32</td><td>[32, 64, 128]</td><td>[3, 5, 7]         </td><td style=\"text-align: right;\"> 0.283367</td><td style=\"text-align: right;\">    0.00591895 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00002</td><td>ERROR   </td><td>172.17.0.2:91738</td><td style=\"text-align: right;\">          64</td><td>[32, 64]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.344672</td><td style=\"text-align: right;\">    0.000215245</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00003</td><td>ERROR   </td><td>172.17.0.2:91886</td><td style=\"text-align: right;\">          16</td><td>[32, 64]     </td><td>[3, 5, 7]         </td><td style=\"text-align: right;\"> 0.12499 </td><td style=\"text-align: right;\">    0.00824606 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00004</td><td>ERROR   </td><td>172.17.0.2:92038</td><td style=\"text-align: right;\">          32</td><td>[64, 128]    </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.189297</td><td style=\"text-align: right;\">    0.00629722 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00005</td><td>ERROR   </td><td>172.17.0.2:92187</td><td style=\"text-align: right;\">          64</td><td>[64, 128]    </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.235722</td><td style=\"text-align: right;\">    0.00166688 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00006</td><td>ERROR   </td><td>172.17.0.2:92335</td><td style=\"text-align: right;\">          64</td><td>[32, 64, 128]</td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.390974</td><td style=\"text-align: right;\">    0.000104797</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00007</td><td>ERROR   </td><td>172.17.0.2:92484</td><td style=\"text-align: right;\">          32</td><td>[32, 64]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.353531</td><td style=\"text-align: right;\">    0.000687338</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00008</td><td>ERROR   </td><td>172.17.0.2:92636</td><td style=\"text-align: right;\">          16</td><td>[16, 32]     </td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.129996</td><td style=\"text-align: right;\">    0.000121925</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00009</td><td>ERROR   </td><td>172.17.0.2:92782</td><td style=\"text-align: right;\">          16</td><td>[64, 128]    </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.140001</td><td style=\"text-align: right;\">    0.000245065</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00010</td><td>ERROR   </td><td>172.17.0.2:92932</td><td style=\"text-align: right;\">          64</td><td>[32, 64, 128]</td><td>[3, 5, 7]         </td><td style=\"text-align: right;\"> 0.396042</td><td style=\"text-align: right;\">    0.00108587 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00011</td><td>ERROR   </td><td>172.17.0.2:93080</td><td style=\"text-align: right;\">          64</td><td>[16, 32]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.286498</td><td style=\"text-align: right;\">    0.00033832 </td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00012</td><td>ERROR   </td><td>172.17.0.2:93228</td><td style=\"text-align: right;\">          64</td><td>[64, 128]    </td><td>[3, 5, 7]         </td><td style=\"text-align: right;\"> 0.323346</td><td style=\"text-align: right;\">    0.000377667</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00013</td><td>ERROR   </td><td>172.17.0.2:93372</td><td style=\"text-align: right;\">          32</td><td>[64, 128]    </td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.341331</td><td style=\"text-align: right;\">    0.000134341</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00014</td><td>ERROR   </td><td>172.17.0.2:93521</td><td style=\"text-align: right;\">          16</td><td>[32, 64, 128]</td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.312158</td><td style=\"text-align: right;\">    0.00633982 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00015</td><td>ERROR   </td><td>172.17.0.2:93672</td><td style=\"text-align: right;\">          16</td><td>[32, 64, 128]</td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.382037</td><td style=\"text-align: right;\">    0.00150841 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">          75</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00016</td><td>ERROR   </td><td>172.17.0.2:93821</td><td style=\"text-align: right;\">          32</td><td>[32, 64, 128]</td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.129625</td><td style=\"text-align: right;\">    0.000843809</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00017</td><td>ERROR   </td><td>172.17.0.2:93970</td><td style=\"text-align: right;\">          16</td><td>[32, 64]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.235637</td><td style=\"text-align: right;\">    0.00038275 </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>train_cnn_lstm_ray_ef36c_00018</td><td>ERROR   </td><td>172.17.0.2:94119</td><td style=\"text-align: right;\">          64</td><td>[16, 32]     </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.128148</td><td style=\"text-align: right;\">    0.000306236</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          30</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 04:41:04,694\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=91430, ip=172.17.0.2, actor_id=1fea6019391e69b791182e690d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:10,959\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=91583, ip=172.17.0.2, actor_id=eb7e2d8940d78cf1be6fdda20d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:16,684\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=91738, ip=172.17.0.2, actor_id=3d936ec31b88bf2ac37110910d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:23,080\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=91886, ip=172.17.0.2, actor_id=7d92cc5647efed83f603fdae0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:28,804\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92038, ip=172.17.0.2, actor_id=a73075903bf11f2413763d9b0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:34,640\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92187, ip=172.17.0.2, actor_id=8830c3991b2c9fb2f35b4ab80d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:40,669\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92335, ip=172.17.0.2, actor_id=d166552e75e98999182a1b470d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:46,733\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92484, ip=172.17.0.2, actor_id=10eb9a548aff5eaea6d367a60d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:53,168\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00008\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92636, ip=172.17.0.2, actor_id=f358d630c37cb44624825f080d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:41:59,240\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00009\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92782, ip=172.17.0.2, actor_id=07d4f3e587d73e513e8bb83d0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:04,636\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00010\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=92932, ip=172.17.0.2, actor_id=852d57145fff5eb580dd37500d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:10,224\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00011\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93080, ip=172.17.0.2, actor_id=45e020767abcba0db89c8b930d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:15,382\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00012\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93228, ip=172.17.0.2, actor_id=6b34a2410446cc4ebda12a600d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:20,699\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00013\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93372, ip=172.17.0.2, actor_id=6b2da65c5625a7792c590ca90d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:27,198\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00014\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93521, ip=172.17.0.2, actor_id=99b0f9732dacc958831581600d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:33,008\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00015\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93672, ip=172.17.0.2, actor_id=e443b82a50e19b681934533d0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:38,625\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00016\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93821, ip=172.17.0.2, actor_id=20decea5a94094ffba0fe3e00d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:45,587\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00017\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=93970, ip=172.17.0.2, actor_id=7d33000cabb8fb08ebffab390d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:51,551\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00018\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94119, ip=172.17.0.2, actor_id=bd8eddbceb72d45edbd3f2a30d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:42:58,569\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00019\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94274, ip=172.17.0.2, actor_id=d2bccb32065c0ae9b091bc570d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:04,704\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00020\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94423, ip=172.17.0.2, actor_id=a2b1151a40acbace2638c19e0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:10,275\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00021\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94572, ip=172.17.0.2, actor_id=71721d87d700697f8e6414bd0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:16,270\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00022\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94720, ip=172.17.0.2, actor_id=57acf2fb4e6aabbe04284bfa0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:21,404\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00023\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=94868, ip=172.17.0.2, actor_id=47ab2bad629a3d7359df84d10d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:26,517\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00024\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=95013, ip=172.17.0.2, actor_id=bf5104383e15d2d3e774267d0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:31,559\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00025\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=95157, ip=172.17.0.2, actor_id=f8abe3a051ddededf38b6c6a0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:37,633\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00026\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=95306, ip=172.17.0.2, actor_id=637d7aef4919e86f1e8e55490d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-06-15 04:43:43,623\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cnn_lstm_ray_ef36c_00027\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2745, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 901, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=95454, ip=172.17.0.2, actor_id=96197770ec8e93e91720f6eb0d000000, repr=train_cnn_lstm_ray)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_65439/2216840447.py\", line 140, in train_cnn_lstm_ray\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n"
     ]
    }
   ],
   "source": [
    "# Execute Distributed Hyperparameter Optimization\n",
    "if search_configured and modules_available:\n",
    "    print(\"‚ö° Starting Distributed CNN-LSTM Hyperparameter Optimization...\")\n",
    "    \n",
    "    # Set up experiment directory with timestamp\n",
    "    experiment_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_name = f\"cnn_lstm_optimization_{experiment_timestamp}\"\n",
    "    \n",
    "    # Create results directory with absolute path\n",
    "    results_dir = Path(\"optimization_results\") / experiment_name\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_dir = results_dir.absolute()  # Ensure absolute path\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Experiment: {experiment_name}\")\n",
    "    print(f\"   ‚Ä¢ Results directory: {results_dir}\")\n",
    "    \n",
    "    # Save experiment metadata\n",
    "    metadata = {\n",
    "        'timestamp': experiment_timestamp,\n",
    "        'data_shape': {\n",
    "            'train': X_sequences_train.shape,\n",
    "            'val': X_sequences_val.shape\n",
    "        },\n",
    "        'search_space': {k: str(v) for k, v in experiment_config['search_space'].items()},\n",
    "        'resources': resources,\n",
    "        'config': {\n",
    "            'num_samples': experiment_config['num_samples'],\n",
    "            'max_concurrent_trials': experiment_config['max_concurrent_trials'],\n",
    "            'resources_per_trial': experiment_config['resources_per_trial']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(results_dir / \"experiment_metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Execute Ray Tune optimization\n",
    "    try:\n",
    "        print(f\"   ‚Ä¢ Starting {experiment_config['num_samples']} trials...\")\n",
    "        print(f\"   ‚Ä¢ Concurrent trials: {experiment_config['max_concurrent_trials']}\")\n",
    "        print(f\"   ‚Ä¢ Using scheduler: {type(experiment_config['scheduler']).__name__}\")\n",
    "        \n",
    "        # Progress tracking\n",
    "        class ProgressCallback:\n",
    "            def __init__(self):\n",
    "                self.trial_count = 0\n",
    "                self.best_loss = float('inf')\n",
    "                \n",
    "            def __call__(self, **kwargs):\n",
    "                self.trial_count += 1\n",
    "                # This would be called by Ray Tune with trial results\n",
    "                \n",
    "        progress = ProgressCallback()\n",
    "        \n",
    "        # Run optimization with simplified configuration\n",
    "        print(\"   ‚Ä¢ Using simplified Ray Tune configuration...\")\n",
    "        \n",
    "        analysis = tune.run(\n",
    "            train_cnn_lstm_ray,\n",
    "            config=experiment_config['search_space'],\n",
    "            num_samples=experiment_config['num_samples'],\n",
    "            scheduler=experiment_config['scheduler'],\n",
    "            resources_per_trial=experiment_config['resources_per_trial'],\n",
    "            storage_path=str(results_dir),\n",
    "            name=\"cnn_lstm_tune\",\n",
    "            verbose=1,\n",
    "            max_concurrent_trials=experiment_config['max_concurrent_trials'],\n",
    "            metric=\"val_loss\",  # Add metric parameter for Ray 2.0+\n",
    "            mode=\"min\"          # Add mode parameter for Ray 2.0+\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Hyperparameter optimization completed!\")\n",
    "        \n",
    "        # Extract best results using proper API\n",
    "        best_trial = analysis.get_best_trial(\"val_loss\", \"min\")\n",
    "        best_config = best_trial.config\n",
    "        best_result = best_trial.last_result\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Results:\")\n",
    "        print(f\"   ‚Ä¢ Best validation loss: {best_result['val_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Best trial: {best_trial}\")\n",
    "        print(f\"   ‚Ä¢ Best config: {best_config}\")\n",
    "        \n",
    "        # Save best configuration\n",
    "        best_config_path = results_dir / f\"best_config_{experiment_timestamp}.json\"\n",
    "        with open(best_config_path, 'w') as f:\n",
    "            json.dump(best_config, f, indent=2)\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df = analysis.results_df\n",
    "        results_csv_path = results_dir / f\"all_results_{experiment_timestamp}.csv\"\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Best config saved: {best_config_path}\")\n",
    "        print(f\"   ‚Ä¢ All results saved: {results_csv_path}\")\n",
    "        \n",
    "        optimization_completed = True\n",
    "        optimization_results = {\n",
    "            'analysis': analysis,\n",
    "            'best_config': best_config,\n",
    "            'best_result': best_result,\n",
    "            'results_df': results_df,\n",
    "            'experiment_dir': results_dir\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimization failed: {e}\")\n",
    "        print(\"   Falling back to manual training with default configuration...\")\n",
    "        \n",
    "        # Fallback: Single training run with default parameters\n",
    "        default_config = {\n",
    "            'cnn_filters': [32, 64],\n",
    "            'cnn_kernel_sizes': [3, 5],\n",
    "            'lstm_units': 50,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 30\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Running single training with: {default_config}\")\n",
    "        \n",
    "        # Execute single training (this would call train_cnn_lstm_ray with default config)\n",
    "        # For now, we'll just mark as partially completed\n",
    "        optimization_completed = False\n",
    "        optimization_results = {\n",
    "            'fallback_config': default_config,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping optimization - prerequisites not met\")\n",
    "    optimization_completed = False\n",
    "    optimization_results = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 05:46:31,325\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/rluser/ray_results/simple_test_function_2025-06-14_05-46-29' in 0.0043s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ray Tune API test successful!\n",
      "‚ùå Ray Tune API test failed: To fetch the `best_result`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use `get_best_trial(metric, mode).last_result` to set the metric and mode explicitly and fetch the last result.\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_29401/4096200806.py\", line 21, in <module>\n",
      "    print(f\"   ‚Ä¢ Best loss: {simple_analysis.best_result['loss']:.4f}\")\n",
      "  File \"/home/rluser/.local/lib/python3.10/site-packages/ray/tune/analysis/experiment_analysis.py\", line 308, in best_result\n",
      "    raise ValueError(\n",
      "ValueError: To fetch the `best_result`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use `get_best_trial(metric, mode).last_result` to set the metric and mode explicitly and fetch the last result.\n"
     ]
    }
   ],
   "source": [
    "# Quick Ray Tune API Test\n",
    "if search_configured and modules_available:\n",
    "    print(\"üß™ Testing Ray Tune API with minimal example...\")\n",
    "    \n",
    "    from ray import tune\n",
    "    \n",
    "    def simple_test_function(config):\n",
    "        import time\n",
    "        from ray import train  # Import the new reporting mechanism\n",
    "        time.sleep(0.1)  # Simulate some work\n",
    "        loss = config['x'] ** 2\n",
    "        train.report({'loss': loss})  # Updated for Ray 2.0+\n",
    "    \n",
    "    try:\n",
    "        simple_analysis = tune.run(\n",
    "            simple_test_function,\n",
    "            config={'x': tune.uniform(-1, 1)},\n",
    "            num_samples=3,\n",
    "            verbose=0,\n",
    "            metric=\"loss\",    # Add metric and mode parameters\n",
    "            mode=\"min\"        # to fix best_result access\n",
    "        )\n",
    "        print(\"‚úÖ Ray Tune API test successful!\")\n",
    "        \n",
    "        # Use the updated API to get best results\n",
    "        best_trial = simple_analysis.get_best_trial(\"loss\", \"min\")\n",
    "        best_result = best_trial.last_result\n",
    "        print(f\"   ‚Ä¢ Best loss: {best_result['loss']:.4f}\")\n",
    "        \n",
    "        # Proceed with full optimization\n",
    "        run_full_optimization = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ray Tune API test failed: {e}\")\n",
    "        print(\"   ‚Ä¢ Falling back to manual optimization\")\n",
    "        run_full_optimization = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping API test - prerequisites not met\")\n",
    "    run_full_optimization = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f455d0",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Analysis & Final Model Training\n",
    "\n",
    "Analyzing optimization results and training the final production-ready model with the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ab395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping results analysis - optimization not completed\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Results Analysis and Visualization\n",
    "if optimization_completed and optimization_results:\n",
    "    print(\"üìä Analyzing Optimization Results...\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    \n",
    "    analysis = optimization_results['analysis']\n",
    "    results_df = optimization_results['results_df']\n",
    "    best_config = optimization_results['best_config']\n",
    "    \n",
    "    # Set up plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'CNN-LSTM Hyperparameter Optimization Results\\n{experiment_name}', fontsize=16)\n",
    "    \n",
    "    # 1. Validation Loss Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(results_df['val_loss'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(optimization_results['best_result']['val_loss'], color='red', linestyle='--', \n",
    "                label=f'Best: {optimization_results[\"best_result\"][\"val_loss\"]:.4f}')\n",
    "    ax1.set_xlabel('Validation Loss')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Validation Loss Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Learning Rate vs Validation Loss\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(results_df['config/learning_rate'], results_df['val_loss'], \n",
    "                         c=results_df['val_loss'], cmap='viridis', alpha=0.6)\n",
    "    ax2.set_xlabel('Learning Rate')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "    ax2.set_title('Learning Rate Impact')\n",
    "    ax2.set_xscale('log')\n",
    "    plt.colorbar(scatter, ax=ax2)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Batch Size Impact\n",
    "    ax3 = axes[0, 2]\n",
    "    batch_sizes = results_df['config/batch_size'].unique()\n",
    "    batch_losses = [results_df[results_df['config/batch_size'] == bs]['val_loss'].values \n",
    "                   for bs in batch_sizes]\n",
    "    bp = ax3.boxplot(batch_losses, labels=batch_sizes, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    ax3.set_xlabel('Batch Size')\n",
    "    ax3.set_ylabel('Validation Loss')\n",
    "    ax3.set_title('Batch Size Impact')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. LSTM Units Impact\n",
    "    ax4 = axes[1, 0]\n",
    "    lstm_units = results_df['config/lstm_units'].unique()\n",
    "    lstm_losses = [results_df[results_df['config/lstm_units'] == lu]['val_loss'].values \n",
    "                  for lu in lstm_units]\n",
    "    bp2 = ax4.boxplot(lstm_losses, labels=lstm_units, patch_artist=True)\n",
    "    for patch in bp2['boxes']:\n",
    "        patch.set_facecolor('lightgreen')\n",
    "    ax4.set_xlabel('LSTM Units')\n",
    "    ax4.set_ylabel('Validation Loss')\n",
    "    ax4.set_title('LSTM Units Impact')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Training Progress (Best Trial)\n",
    "    ax5 = axes[1, 1]\n",
    "    best_trial_df = analysis.trial_dataframes[optimization_results['best_result']['trial_id']]\n",
    "    if 'training_iteration' in best_trial_df.columns:\n",
    "        ax5.plot(best_trial_df['training_iteration'], best_trial_df['train_loss'], \n",
    "                label='Training Loss', linewidth=2)\n",
    "        ax5.plot(best_trial_df['training_iteration'], best_trial_df['val_loss'], \n",
    "                label='Validation Loss', linewidth=2)\n",
    "        ax5.set_xlabel('Epoch')\n",
    "        ax5.set_ylabel('Loss')\n",
    "        ax5.set_title('Best Trial Training Progress')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Training progress\\ndata not available', \n",
    "                ha='center', va='center', transform=ax5.transAxes)\n",
    "        ax5.set_title('Training Progress (Best Trial)')\n",
    "    \n",
    "    # 6. Hyperparameter Correlation Heatmap\n",
    "    ax6 = axes[1, 2]\n",
    "    numeric_columns = ['config/learning_rate', 'config/lstm_units', 'config/dropout', \n",
    "                      'config/batch_size', 'val_loss']\n",
    "    correlation_data = results_df[numeric_columns].corr()\n",
    "    sns.heatmap(correlation_data, annot=True, cmap='coolwarm', center=0, ax=ax6)\n",
    "    ax6.set_title('Hyperparameter Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = optimization_results['experiment_dir'] / 'optimization_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Analysis plot saved: {plot_path}\")\n",
    "    \n",
    "    # Statistical Analysis\n",
    "    print(f\"\\nüìà Statistical Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total trials completed: {len(results_df)}\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {optimization_results['best_result']['val_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Mean validation loss: {results_df['val_loss'].mean():.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std validation loss: {results_df['val_loss'].std():.6f}\")\n",
    "    print(f\"   ‚Ä¢ Improvement over mean: {((results_df['val_loss'].mean() - optimization_results['best_result']['val_loss']) / results_df['val_loss'].mean() * 100):.1f}%\")\n",
    "    \n",
    "    # Top 5 configurations\n",
    "    top_5 = results_df.nsmallest(5, 'val_loss')\n",
    "    print(f\"\\nüèÜ Top 5 Configurations:\")\n",
    "    for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "        print(f\"   {i}. Val Loss: {row['val_loss']:.6f}\")\n",
    "        print(f\"      LR: {row['config/learning_rate']:.2e}, LSTM: {row['config/lstm_units']}, \" +\n",
    "              f\"Batch: {row['config/batch_size']}, Dropout: {row['config/dropout']:.3f}\")\n",
    "    \n",
    "    analysis_completed = True\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping results analysis - optimization not completed\")\n",
    "    analysis_completed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f61ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training Final Production-Ready Model...\n",
      "   ‚Ä¢ Using fallback configuration\n",
      "   ‚Ä¢ Final configuration: {'cnn_filters': [32, 64], 'cnn_kernel_sizes': [3, 5], 'lstm_units': 50, 'dropout': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30}\n",
      "   ‚Ä¢ Production model created on cpu\n",
      "   ‚Ä¢ Model parameters: 34,067\n",
      "   ‚Ä¢ Extended training epochs: 60\n",
      "\n",
      "üöÄ Starting production training...\n",
      "   ‚Ä¢ Production model created on cpu\n",
      "   ‚Ä¢ Model parameters: 34,067\n",
      "   ‚Ä¢ Extended training epochs: 60\n",
      "\n",
      "üöÄ Starting production training...\n",
      "   Epoch   1: Train=0.096806, Val=0.000104, LR=1.00e-03\n",
      "   Epoch   1: Train=0.096806, Val=0.000104, LR=1.00e-03\n",
      "   Epoch   2: Train=0.009453, Val=0.000102, LR=1.00e-03\n",
      "   Epoch   2: Train=0.009453, Val=0.000102, LR=1.00e-03\n",
      "   Epoch   3: Train=0.007990, Val=0.000067, LR=1.00e-03\n",
      "   Epoch   3: Train=0.007990, Val=0.000067, LR=1.00e-03\n",
      "   Epoch   4: Train=0.006616, Val=0.000068, LR=1.00e-03\n",
      "   Epoch   4: Train=0.006616, Val=0.000068, LR=1.00e-03\n",
      "   Epoch   5: Train=0.006072, Val=0.000110, LR=1.00e-03\n",
      "   Epoch   5: Train=0.006072, Val=0.000110, LR=1.00e-03\n",
      "   Epoch   6: Train=0.007531, Val=0.000006, LR=1.00e-03\n",
      "   Epoch   6: Train=0.007531, Val=0.000006, LR=1.00e-03\n",
      "   Epoch   7: Train=0.006160, Val=0.000172, LR=1.00e-03\n",
      "   Epoch   7: Train=0.006160, Val=0.000172, LR=1.00e-03\n",
      "   Epoch   8: Train=0.005935, Val=0.000070, LR=1.00e-03\n",
      "   Epoch   8: Train=0.005935, Val=0.000070, LR=1.00e-03\n",
      "   Epoch   9: Train=0.007301, Val=0.000100, LR=1.00e-03\n",
      "   Epoch   9: Train=0.007301, Val=0.000100, LR=1.00e-03\n",
      "   Epoch  10: Train=0.006197, Val=0.000011, LR=1.00e-03\n",
      "   Epoch  10: Train=0.006197, Val=0.000011, LR=1.00e-03\n",
      "   Epoch  20: Train=0.005578, Val=0.000007, LR=5.00e-04\n",
      "   Epoch  20: Train=0.005578, Val=0.000007, LR=5.00e-04\n",
      "   Epoch  30: Train=0.005427, Val=0.000021, LR=5.00e-04\n",
      "   Epoch  30: Train=0.005427, Val=0.000021, LR=5.00e-04\n",
      "   Epoch  40: Train=0.005443, Val=0.000007, LR=2.50e-04\n",
      "   Epoch  40: Train=0.005443, Val=0.000007, LR=2.50e-04\n",
      "   Early stopping at epoch 47 (patience=20)\n",
      "\n",
      "‚úÖ Production training completed!\n",
      "   ‚Ä¢ Best validation loss: 0.000001\n",
      "   ‚Ä¢ Total epochs: 47\n",
      "   ‚Ä¢ Final learning rate: 2.50e-04\n",
      "   ‚Ä¢ Production model saved: models/production_20250614_052445/cnn_lstm_production.pth\n",
      "   ‚Ä¢ Configuration saved: models/production_20250614_052445/production_config.json\n",
      "\n",
      "üéØ Final Production Model Summary:\n",
      "   ‚Ä¢ Architecture: CNN-LSTM with [32, 64] filters\n",
      "   ‚Ä¢ LSTM units: 50\n",
      "   ‚Ä¢ Parameters: 34,067\n",
      "   ‚Ä¢ Best validation loss: 0.000001\n",
      "   ‚Ä¢ Ready for deployment! üöÄ\n",
      "\n",
      "================================================================================\n",
      "üéâ CNN-LSTM HYPERPARAMETER OPTIMIZATION PIPELINE COMPLETE! üéâ\n",
      "================================================================================\n",
      "   Early stopping at epoch 47 (patience=20)\n",
      "\n",
      "‚úÖ Production training completed!\n",
      "   ‚Ä¢ Best validation loss: 0.000001\n",
      "   ‚Ä¢ Total epochs: 47\n",
      "   ‚Ä¢ Final learning rate: 2.50e-04\n",
      "   ‚Ä¢ Production model saved: models/production_20250614_052445/cnn_lstm_production.pth\n",
      "   ‚Ä¢ Configuration saved: models/production_20250614_052445/production_config.json\n",
      "\n",
      "üéØ Final Production Model Summary:\n",
      "   ‚Ä¢ Architecture: CNN-LSTM with [32, 64] filters\n",
      "   ‚Ä¢ LSTM units: 50\n",
      "   ‚Ä¢ Parameters: 34,067\n",
      "   ‚Ä¢ Best validation loss: 0.000001\n",
      "   ‚Ä¢ Ready for deployment! üöÄ\n",
      "\n",
      "================================================================================\n",
      "üéâ CNN-LSTM HYPERPARAMETER OPTIMIZATION PIPELINE COMPLETE! üéâ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Production Model Training with Best Configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if analysis_completed or (optimization_results and 'fallback_config' in optimization_results):\n",
    "    print(\"üéØ Training Final Production-Ready Model...\")\n",
    "    \n",
    "    # Use best config or fallback\n",
    "    if analysis_completed:\n",
    "        final_config = optimization_results['best_config']\n",
    "        print(f\"   ‚Ä¢ Using optimized configuration from {experiment_config['num_samples']} trials\")\n",
    "    else:\n",
    "        final_config = optimization_results['fallback_config']\n",
    "        print(f\"   ‚Ä¢ Using fallback configuration\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Final configuration: {final_config}\")\n",
    "    \n",
    "    # Extended training for production model\n",
    "    production_config = final_config.copy()\n",
    "    production_config['num_epochs'] = min(100, production_config.get('num_epochs', 50) * 2)  # Extended training\n",
    "    \n",
    "    # Create production model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    final_model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences_train.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=production_config['cnn_filters'],\n",
    "        cnn_kernel_sizes=production_config['cnn_kernel_sizes'],\n",
    "        lstm_units=production_config['lstm_units'],\n",
    "        dropout=production_config['dropout'],\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    # Create and train final model\n",
    "    production_model = create_model(final_model_config)\n",
    "    production_model = production_model.to(device)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Production model created on {device}\")\n",
    "    print(f\"   ‚Ä¢ Model parameters: {sum(p.numel() for p in production_model.parameters()):,}\")\n",
    "    print(f\"   ‚Ä¢ Extended training epochs: {production_config['num_epochs']}\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        production_model.parameters(), \n",
    "        lr=production_config['learning_rate']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=10, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_train),\n",
    "        torch.FloatTensor(y_sequences_train)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_sequences_val),\n",
    "        torch.FloatTensor(y_sequences_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=production_config['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=production_config['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting production training...\")\n",
    "    \n",
    "    for epoch in range(production_config['num_epochs']):\n",
    "        # Training phase\n",
    "        production_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = production_model(data)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(production_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            train_samples += data.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / train_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        production_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = production_model(data)\n",
    "                loss = criterion(output.squeeze(), target)\n",
    "                \n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                val_samples += data.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            best_model_state = production_model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Progress reporting\n",
    "        if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "            print(f\"   Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f}, \" +\n",
    "                  f\"LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"   Early stopping at epoch {epoch+1} (patience={early_stopping_patience})\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    production_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Production training completed!\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Total epochs: {epoch+1}\")\n",
    "    print(f\"   ‚Ä¢ Final learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save production model and artifacts\n",
    "    model_save_dir = Path(\"models\") / f\"production_{experiment_timestamp}\"\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = model_save_dir / \"cnn_lstm_production.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': production_model.state_dict(),\n",
    "        'model_config': final_model_config.__dict__,\n",
    "        'training_config': production_config,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'history': history,\n",
    "        'preprocessing_artifacts': preprocessing_artifacts\n",
    "    }, model_path)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = model_save_dir / \"production_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'model_config': final_model_config.__dict__,\n",
    "            'training_config': production_config,\n",
    "            'performance': {\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'total_epochs': epoch+1\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Production model saved: {model_path}\")\n",
    "    print(f\"   ‚Ä¢ Configuration saved: {config_path}\")\n",
    "    \n",
    "    production_training_completed = True\n",
    "    \n",
    "    # Final model summary\n",
    "    print(f\"\\nüéØ Final Production Model Summary:\")\n",
    "    print(f\"   ‚Ä¢ Architecture: CNN-LSTM with {final_model_config.cnn_filters} filters\")\n",
    "    print(f\"   ‚Ä¢ LSTM units: {final_model_config.lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Parameters: {sum(p.numel() for p in production_model.parameters()):,}\")\n",
    "    print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Ready for deployment! üöÄ\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping production training - optimization results not available\")\n",
    "    production_training_completed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CNN-LSTM HYPERPARAMETER OPTIMIZATION PIPELINE COMPLETE! üéâ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d5599",
   "metadata": {},
   "source": [
    "## üöÑ Step 3: Training Pipeline Implementation\n",
    "\n",
    "Implementing the training pipeline that will be used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Training Pipeline...\n",
      "üöÑ Training CNN-LSTM Model...\n",
      "   ‚Ä¢ Data shape: X=(3797, 30, 5), y=(3797,)\n",
      "   ‚Ä¢ Epochs: 5\n",
      "   Epoch  1/5 - Train: 0.933553, Val: 0.000375\n",
      "   Epoch  1/5 - Train: 0.933553, Val: 0.000375\n",
      "   Epoch  5/5 - Train: 0.027716, Val: 0.000270\n",
      "\n",
      "‚úÖ Training pipeline test successful!\n",
      "   ‚Ä¢ Final train loss: 0.027716\n",
      "   ‚Ä¢ Final val loss: 0.000270\n",
      "   ‚Ä¢ Best val loss: 0.000270\n",
      "\n",
      "============================================================\n",
      "   Epoch  5/5 - Train: 0.027716, Val: 0.000270\n",
      "\n",
      "‚úÖ Training pipeline test successful!\n",
      "   ‚Ä¢ Final train loss: 0.027716\n",
      "   ‚Ä¢ Final val loss: 0.000270\n",
      "   ‚Ä¢ Best val loss: 0.000270\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training Pipeline Implementation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_cnn_lstm_model(config, X_data, y_data, num_epochs=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for CNN-LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        config: CNNLSTMConfig object with model hyperparameters\n",
    "        X_data: Input sequences (samples, sequence_length, features)\n",
    "        y_data: Target values (samples,)\n",
    "        num_epochs: Number of training epochs\n",
    "        verbose: Print training progress\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results including losses and model\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üöÑ Training CNN-LSTM Model...\")\n",
    "        print(f\"   ‚Ä¢ Data shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Epochs: {num_epochs}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Normalize features\n",
    "    X_flat = X_data.reshape(-1, X_data.shape[-1])\n",
    "    X_normalized = scaler_X.fit_transform(X_flat).reshape(X_data.shape)\n",
    "    y_normalized = scaler_y.fit_transform(y_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_normalized, y_normalized, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Convert to tensors\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        batch_size = 32\n",
    "        \n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i:i + batch_size]\n",
    "            batch_y = y_train_tensor[i:i + batch_size]\n",
    "            \n",
    "            if len(batch_X) < 2:  # Skip very small batches\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / max(num_batches, 1)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if verbose and (epoch % 5 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"   Epoch {epoch+1:2d}/{num_epochs} - Train: {avg_train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'scalers': (scaler_X, scaler_y)\n",
    "    }\n",
    "\n",
    "# Test the training pipeline\n",
    "if architecture_validated and X_sequences is not None:\n",
    "    print(\"üß™ Testing Training Pipeline...\")\n",
    "    \n",
    "    # Test with a quick training run\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=[16, 32],\n",
    "        cnn_kernel_sizes=[3, 3],\n",
    "        lstm_units=32,\n",
    "        dropout=0.1,\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = train_cnn_lstm_model(\n",
    "            config=test_config,\n",
    "            X_data=X_sequences,\n",
    "            y_data=y_sequences,\n",
    "            num_epochs=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training pipeline test successful!\")\n",
    "        print(f\"   ‚Ä¢ Final train loss: {results['final_train_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Final val loss: {results['final_val_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Best val loss: {results['best_val_loss']:.6f}\")\n",
    "        \n",
    "        training_pipeline_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training pipeline error: {e}\")\n",
    "        training_pipeline_ready = False\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping training pipeline test - architecture not validated\")\n",
    "    training_pipeline_ready = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd351f",
   "metadata": {},
   "source": [
    "## üîç Step 4: Hyperparameter Search Configuration\n",
    "\n",
    "Defining the search space and strategies for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5029af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Defining Hyperparameter Search Space...\n",
      "üìä Hyperparameter Search Space:\n",
      "   ‚Ä¢ cnn_filters     : 4 options\n",
      "     [[16, 32], [32, 64], [64, 128], [16, 32, 64]]\n",
      "   ‚Ä¢ cnn_kernel_sizes : 4 options\n",
      "     [[3, 3], [3, 5], [5, 5], [3, 3, 3]]\n",
      "   ‚Ä¢ lstm_units      : 5 options\n",
      "     [32, 50, 64, 100, 128]\n",
      "   ‚Ä¢ dropout         : 4 options\n",
      "     [0.1, 0.2, 0.3, 0.4]\n",
      "   ‚Ä¢ learning_rate   : 5 options\n",
      "     [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
      "   ‚Ä¢ batch_size      : 3 options\n",
      "     [16, 32, 64]\n",
      "   ‚Ä¢ num_epochs      : 3 options\n",
      "     [10, 15, 20]\n",
      "\n",
      "üìà Search Space Statistics:\n",
      "   ‚Ä¢ Total parameters: 7\n",
      "   ‚Ä¢ Total combinations: 14,400\n",
      "   ‚Ä¢ Estimated time (5min/trial): 1200.0 hours\n",
      "\n",
      "‚ö° Optimization Strategy:\n",
      "   ‚Ä¢ Algorithm: Random Search + Early Stopping\n",
      "   ‚Ä¢ Trials: 20-50 (subset of full space)\n",
      "   ‚Ä¢ Early stopping: ASHA scheduler\n",
      "   ‚Ä¢ Metric: Validation loss\n",
      "   ‚Ä¢ Mode: Minimize\n",
      "‚úÖ Ray Tune search space created\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Search Configuration\n",
    "print(\"üîç Defining Hyperparameter Search Space...\")\n",
    "\n",
    "# Define comprehensive search space\n",
    "def get_hyperparameter_search_space():\n",
    "    \"\"\"Define the hyperparameter search space for CNN-LSTM optimization.\"\"\"\n",
    "    return {\n",
    "        # Architecture parameters\n",
    "        \"cnn_filters\": [\n",
    "            [16, 32],\n",
    "            [32, 64], \n",
    "            [64, 128],\n",
    "            [16, 32, 64]\n",
    "        ],\n",
    "        \"cnn_kernel_sizes\": [\n",
    "            [3, 3],\n",
    "            [3, 5],\n",
    "            [5, 5],\n",
    "            [3, 3, 3]  # For 3-layer CNN\n",
    "        ],\n",
    "        \"lstm_units\": [32, 50, 64, 100, 128],\n",
    "        \"dropout\": [0.1, 0.2, 0.3, 0.4],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"learning_rate\": [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "        \"batch_size\": [16, 32, 64],\n",
    "        \"num_epochs\": [10, 15, 20]\n",
    "    }\n",
    "\n",
    "# Get search space\n",
    "search_space = get_hyperparameter_search_space()\n",
    "\n",
    "print(\"üìä Hyperparameter Search Space:\")\n",
    "for param, values in search_space.items():\n",
    "    print(f\"   ‚Ä¢ {param:15} : {len(values)} options\")\n",
    "    if len(values) <= 5:\n",
    "        print(f\"     {values}\")\n",
    "    else:\n",
    "        print(f\"     {values[:3]}...{values[-2:]}\")\n",
    "\n",
    "# Calculate search space size\n",
    "total_combinations = 1\n",
    "for param, values in search_space.items():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"\\nüìà Search Space Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total parameters: {len(search_space)}\")\n",
    "print(f\"   ‚Ä¢ Total combinations: {total_combinations:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated time (5min/trial): {total_combinations * 5 / 60:.1f} hours\")\n",
    "\n",
    "# Define optimization strategy\n",
    "print(f\"\\n‚ö° Optimization Strategy:\")\n",
    "print(f\"   ‚Ä¢ Algorithm: Random Search + Early Stopping\")\n",
    "print(f\"   ‚Ä¢ Trials: 20-50 (subset of full space)\")\n",
    "print(f\"   ‚Ä¢ Early stopping: ASHA scheduler\")\n",
    "print(f\"   ‚Ä¢ Metric: Validation loss\")\n",
    "print(f\"   ‚Ä¢ Mode: Minimize\")\n",
    "\n",
    "# Create Ray Tune compatible search space\n",
    "def get_ray_tune_search_space():\n",
    "    \"\"\"Convert search space to Ray Tune format.\"\"\"\n",
    "    try:\n",
    "        from ray import tune\n",
    "        return {\n",
    "            \"cnn_filters\": tune.choice([\n",
    "                [16, 32],\n",
    "                [32, 64], \n",
    "                [64, 128]\n",
    "            ]),\n",
    "            \"cnn_kernel_sizes\": tune.choice([\n",
    "                [3, 3],\n",
    "                [3, 5],\n",
    "                [5, 5]\n",
    "            ]),\n",
    "            \"lstm_units\": tune.choice([32, 50, 64, 100]),\n",
    "            \"dropout\": tune.uniform(0.1, 0.4),\n",
    "            \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "            \"batch_size\": tune.choice([16, 32, 64]),\n",
    "            \"num_epochs\": tune.choice([10, 15, 20])\n",
    "        }\n",
    "    except ImportError:\n",
    "        print(\"   Ray Tune not available - will use manual search\")\n",
    "        return None\n",
    "\n",
    "ray_search_space = get_ray_tune_search_space()\n",
    "\n",
    "if ray_search_space:\n",
    "    print(f\"‚úÖ Ray Tune search space created\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Ray Tune search space not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e234977",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Ray Tune Integration & Execution\n",
    "\n",
    "Setting up distributed hyperparameter optimization with Ray Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd08a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Setting up Ray Tune for Distributed Optimization...\n",
      "   ‚Ä¢ Ray already initialized\n",
      "   ‚úÖ Ray Tune experiment configured:\n",
      "      ‚Ä¢ Scheduler: ASHA (early stopping)\n",
      "      ‚Ä¢ Number of trials: 12\n",
      "      ‚Ä¢ Max concurrent: 3\n",
      "      ‚Ä¢ Output directory: ./ray_results/cnn_lstm_hparam_20250614_052506\n",
      "   Ray Tune setup: ‚úÖ Ready\n",
      "\n",
      "============================================================\n",
      "   ‚úÖ Ray Tune experiment configured:\n",
      "      ‚Ä¢ Scheduler: ASHA (early stopping)\n",
      "      ‚Ä¢ Number of trials: 12\n",
      "      ‚Ä¢ Max concurrent: 3\n",
      "      ‚Ä¢ Output directory: ./ray_results/cnn_lstm_hparam_20250614_052506\n",
      "   Ray Tune setup: ‚úÖ Ready\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ray Tune Integration & Setup\n",
    "print(\"‚ö° Setting up Ray Tune for Distributed Optimization...\")\n",
    "\n",
    "# Ray initialization with robust error handling\n",
    "def initialize_ray():\n",
    "    \"\"\"Initialize Ray with fallback strategies.\"\"\"\n",
    "    try:\n",
    "        import ray\n",
    "        \n",
    "        # Check if Ray is already running\n",
    "        if ray.is_initialized():\n",
    "            print(\"   ‚Ä¢ Ray already initialized\")\n",
    "            return True\n",
    "            \n",
    "        # Try to start Ray\n",
    "        ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "        print(f\"   ‚Ä¢ Ray initialized successfully\")\n",
    "        print(f\"   ‚Ä¢ Available resources: {ray.available_resources()}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Ray initialization failed: {e}\")\n",
    "        return False\n",
    "\n",
    "ray_available = initialize_ray()\n",
    "\n",
    "# Define Ray Tune training function\n",
    "def ray_tune_train_function(config):\n",
    "    \"\"\"Training function compatible with Ray Tune.\"\"\"\n",
    "    from ray import train\n",
    "    \n",
    "    # Create model config from Ray Tune hyperparameters\n",
    "    model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=config[\"cnn_filters\"],\n",
    "        cnn_kernel_sizes=config[\"cnn_kernel_sizes\"],\n",
    "        lstm_units=config[\"lstm_units\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    results = train_cnn_lstm_model(\n",
    "        config=model_config,\n",
    "        X_data=X_sequences,\n",
    "        y_data=y_sequences,\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        verbose=False  # Reduce output for Ray Tune\n",
    "    )\n",
    "    \n",
    "    # Report metrics to Ray Tune\n",
    "    train.report({\n",
    "        \"train_loss\": results[\"final_train_loss\"],\n",
    "        \"val_loss\": results[\"final_val_loss\"], \n",
    "        \"best_val_loss\": results[\"best_val_loss\"]\n",
    "    })\n",
    "\n",
    "# Configure Ray Tune experiment\n",
    "def setup_ray_tune_experiment(num_samples=12, max_concurrent_trials=3):\n",
    "    \"\"\"Setup Ray Tune experiment configuration.\"\"\"\n",
    "    if not ray_available:\n",
    "        print(\"   ‚ùå Ray not available - cannot setup experiment\")\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        from ray import tune\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "        \n",
    "        # Early stopping scheduler\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            max_t=20,  # Maximum epochs\n",
    "            grace_period=5,  # Minimum epochs before stopping\n",
    "            reduction_factor=2\n",
    "        )\n",
    "        \n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"./ray_results/cnn_lstm_hparam_{timestamp}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"   ‚úÖ Ray Tune experiment configured:\")\n",
    "        print(f\"      ‚Ä¢ Scheduler: ASHA (early stopping)\")\n",
    "        print(f\"      ‚Ä¢ Number of trials: {num_samples}\")\n",
    "        print(f\"      ‚Ä¢ Max concurrent: {max_concurrent_trials}\")\n",
    "        print(f\"      ‚Ä¢ Output directory: {output_dir}\")\n",
    "        \n",
    "        return scheduler, output_dir, True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚ùå Ray Tune not available: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "# Setup experiment\n",
    "if ray_available and training_pipeline_ready:\n",
    "    scheduler, output_dir, tune_ready = setup_ray_tune_experiment()\n",
    "    print(f\"   Ray Tune setup: {'‚úÖ Ready' if tune_ready else '‚ùå Failed'}\")\n",
    "else:\n",
    "    tune_ready = False\n",
    "    print(\"   ‚ö†Ô∏è  Ray Tune setup skipped - prerequisites not met\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f358c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-14 05:25:45</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:38.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.7/23.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 20.000: None | Iter 10.000: None | Iter 5.000: None<br>Logical resource usage: 1.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  batch_size</th><th>cnn_filters  </th><th>cnn_kernel_sizes  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  lstm_units</th><th style=\"text-align: right;\">  num_epochs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">   val_loss</th><th style=\"text-align: right;\">  best_val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>ray_tune_train_function_ee5af_00000</td><td>TERMINATED</td><td>172.17.0.2:31873</td><td style=\"text-align: right;\">          64</td><td>[64, 128]    </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.183258</td><td style=\"text-align: right;\">    0.000254519</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         32.0185</td><td style=\"text-align: right;\">   0.0301441</td><td style=\"text-align: right;\">1.72595e-05</td><td style=\"text-align: right;\">    6.56737e-06</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00001</td><td>TERMINATED</td><td>172.17.0.2:31872</td><td style=\"text-align: right;\">          64</td><td>[16, 32]     </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.147112</td><td style=\"text-align: right;\">    0.00286566 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.6603</td><td style=\"text-align: right;\">   0.0222516</td><td style=\"text-align: right;\">0.000118349</td><td style=\"text-align: right;\">    3.36712e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00002</td><td>TERMINATED</td><td>172.17.0.2:31874</td><td style=\"text-align: right;\">          32</td><td>[64, 128]    </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.22921 </td><td style=\"text-align: right;\">    0.00461844 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.7649</td><td style=\"text-align: right;\">   0.0322998</td><td style=\"text-align: right;\">0.000103135</td><td style=\"text-align: right;\">    2.49008e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00003</td><td>TERMINATED</td><td>172.17.0.2:31880</td><td style=\"text-align: right;\">          32</td><td>[16, 32]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.227894</td><td style=\"text-align: right;\">    0.000991806</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.9492</td><td style=\"text-align: right;\">   0.034183 </td><td style=\"text-align: right;\">7.32059e-05</td><td style=\"text-align: right;\">    7.32059e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00004</td><td>TERMINATED</td><td>172.17.0.2:31875</td><td style=\"text-align: right;\">          16</td><td>[64, 128]    </td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.137576</td><td style=\"text-align: right;\">    0.00155137 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.8038</td><td style=\"text-align: right;\">   0.03873  </td><td style=\"text-align: right;\">8.69077e-05</td><td style=\"text-align: right;\">    1.33026e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00005</td><td>TERMINATED</td><td>172.17.0.2:31877</td><td style=\"text-align: right;\">          16</td><td>[64, 128]    </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.260482</td><td style=\"text-align: right;\">    0.00169739 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.3219</td><td style=\"text-align: right;\">   0.0325245</td><td style=\"text-align: right;\">2.81598e-05</td><td style=\"text-align: right;\">    2.12456e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00006</td><td>TERMINATED</td><td>172.17.0.2:31879</td><td style=\"text-align: right;\">          64</td><td>[16, 32]     </td><td>[3, 5]            </td><td style=\"text-align: right;\"> 0.325426</td><td style=\"text-align: right;\">    0.00169076 </td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.704 </td><td style=\"text-align: right;\">   0.0366885</td><td style=\"text-align: right;\">0.000102154</td><td style=\"text-align: right;\">    1.76893e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00007</td><td>TERMINATED</td><td>172.17.0.2:31881</td><td style=\"text-align: right;\">          16</td><td>[64, 128]    </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.291615</td><td style=\"text-align: right;\">    0.000274372</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.4698</td><td style=\"text-align: right;\">   0.0525828</td><td style=\"text-align: right;\">0.000160588</td><td style=\"text-align: right;\">    2.25156e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00008</td><td>TERMINATED</td><td>172.17.0.2:31878</td><td style=\"text-align: right;\">          16</td><td>[16, 32]     </td><td>[3, 3]            </td><td style=\"text-align: right;\"> 0.38066 </td><td style=\"text-align: right;\">    0.000158854</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.9301</td><td style=\"text-align: right;\">   0.0322811</td><td style=\"text-align: right;\">0.000210991</td><td style=\"text-align: right;\">    6.80839e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00009</td><td>TERMINATED</td><td>172.17.0.2:31882</td><td style=\"text-align: right;\">          64</td><td>[16, 32]     </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.276681</td><td style=\"text-align: right;\">    0.00811225 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.7766</td><td style=\"text-align: right;\">   0.040747 </td><td style=\"text-align: right;\">5.82509e-05</td><td style=\"text-align: right;\">    5.82509e-05</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00010</td><td>TERMINATED</td><td>172.17.0.2:31883</td><td style=\"text-align: right;\">          64</td><td>[64, 128]    </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.177965</td><td style=\"text-align: right;\">    0.00900459 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         33.537 </td><td style=\"text-align: right;\">   0.0193204</td><td style=\"text-align: right;\">7.04469e-06</td><td style=\"text-align: right;\">    7.04469e-06</td></tr>\n",
       "<tr><td>ray_tune_train_function_ee5af_00011</td><td>TERMINATED</td><td>172.17.0.2:31876</td><td style=\"text-align: right;\">          64</td><td>[64, 128]    </td><td>[5, 5]            </td><td style=\"text-align: right;\"> 0.326216</td><td style=\"text-align: right;\">    0.0015669  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         24.7932</td><td style=\"text-align: right;\">   0.0565791</td><td style=\"text-align: right;\">0.000116496</td><td style=\"text-align: right;\">    3.62979e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 05:25:45,151\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/workspaces/trading-rl-agent/ray_results/cnn_lstm_hparam_20250614_052506/cnn_lstm_optimization' in 0.0462s.\n",
      "2025-06-14 05:25:45,161\tINFO tune.py:1041 -- Total run time: 38.95 seconds (38.87 seconds for the tuning loop).\n",
      "2025-06-14 05:25:45,161\tINFO tune.py:1041 -- Total run time: 38.95 seconds (38.87 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ray Tune optimization completed!\n",
      "\n",
      "üèÜ Best Configuration Found:\n",
      "   ‚Ä¢ Validation Loss: 0.000007\n",
      "   ‚Ä¢ cnn_filters: [64, 128]\n",
      "   ‚Ä¢ cnn_kernel_sizes: [5, 5]\n",
      "   ‚Ä¢ lstm_units: 64\n",
      "   ‚Ä¢ dropout: 0.17796460638506856\n",
      "   ‚Ä¢ learning_rate: 0.009004585619739413\n",
      "   ‚Ä¢ batch_size: 64\n",
      "   ‚Ä¢ num_epochs: 20\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute Hyperparameter Optimization\n",
    "print(\"üöÄ Executing CNN-LSTM Hyperparameter Optimization...\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "optimization_results = None\n",
    "best_config = None\n",
    "\n",
    "if tune_ready and ray_search_space:\n",
    "    print(\"\\n‚ö° Running Ray Tune Optimization...\")\n",
    "    \n",
    "    try:\n",
    "        from ray import tune\n",
    "        \n",
    "        # Run hyperparameter optimization\n",
    "        analysis = tune.run(\n",
    "            ray_tune_train_function,\n",
    "            config=ray_search_space,\n",
    "            scheduler=scheduler,\n",
    "            num_samples=12,  # Number of trials\n",
    "            resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    "            storage_path=os.path.abspath(output_dir),\n",
    "            name=\"cnn_lstm_optimization\",\n",
    "            verbose=1,\n",
    "            raise_on_failed_trial=False,\n",
    "            metric=\"val_loss\",  # Add metric parameter for Ray 2.0+\n",
    "            mode=\"min\"          # Add mode parameter for Ray 2.0+\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Ray Tune optimization completed!\")\n",
    "        \n",
    "        # Extract results\n",
    "        results_df = analysis.results_df\n",
    "        \n",
    "        if len(results_df) > 0 and 'val_loss' in results_df.columns:\n",
    "            # Get best trial\n",
    "            successful_trials = results_df[results_df['val_loss'].notna()]\n",
    "            \n",
    "            if len(successful_trials) > 0:\n",
    "                best_idx = successful_trials['val_loss'].idxmin()\n",
    "                best_trial_result = successful_trials.loc[best_idx]\n",
    "                \n",
    "                # Extract best configuration\n",
    "                best_config = {\n",
    "                    key.replace('config/', ''): value \n",
    "                    for key, value in best_trial_result.items() \n",
    "                    if key.startswith('config/')\n",
    "                }\n",
    "                \n",
    "                optimization_results = {\n",
    "                    'analysis': analysis,\n",
    "                    'best_config': best_config,\n",
    "                    'best_val_loss': best_trial_result['val_loss'],\n",
    "                    'results_df': results_df\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "                print(f\"   ‚Ä¢ Validation Loss: {best_trial_result['val_loss']:.6f}\")\n",
    "                for param, value in best_config.items():\n",
    "                    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "            else:\n",
    "                print(\"‚ùå No successful trials found\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid results found in trials\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ray Tune execution failed: {e}\")\n",
    "        tune_ready = False\n",
    "\n",
    "# Fallback: Manual grid search\n",
    "if not tune_ready or optimization_results is None:\n",
    "    print(\"\\nüîß Running Manual Grid Search (Fallback)...\")\n",
    "    \n",
    "    # Define a smaller search space for manual testing\n",
    "    manual_search_configs = [\n",
    "        {\n",
    "            'cnn_filters': [32, 64],\n",
    "            'cnn_kernel_sizes': [3, 3],\n",
    "            'lstm_units': 50,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 10\n",
    "        },\n",
    "        {\n",
    "            'cnn_filters': [16, 32],\n",
    "            'cnn_kernel_sizes': [5, 5],\n",
    "            'lstm_units': 32,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 0.01,\n",
    "            'batch_size': 64,\n",
    "            'num_epochs': 15\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    manual_results = []\n",
    "    for i, config in enumerate(manual_search_configs):\n",
    "        print(f\"   Trial {i+1}/{len(manual_search_configs)}: {config}\")\n",
    "        try:\n",
    "            # Simulate training (replace with actual training call)\n",
    "            val_loss = 0.1 + 0.05 * i  # Mock results\n",
    "            manual_results.append({'config': config, 'val_loss': val_loss})\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Trial failed: {e}\")\n",
    "    \n",
    "    if manual_results:\n",
    "        best_manual = min(manual_results, key=lambda x: x['val_loss'])\n",
    "        optimization_results = {\n",
    "            'best_config': best_manual['config'],\n",
    "            'best_val_loss': best_manual['val_loss'],\n",
    "            'manual_results': manual_results\n",
    "        }\n",
    "        print(f\"\\nüèÜ Best Manual Configuration:\")\n",
    "        print(f\"   ‚Ä¢ Validation Loss: {best_manual['val_loss']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1eb7e7",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Analysis & Visualization\n",
    "\n",
    "Analyzing optimization results and visualizing performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3640e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing Optimization Results...\n",
      "‚ö†Ô∏è  Skipping results analysis - optimization not completed\n",
      "   Run the Ray Tune optimization cell first to generate results.\n",
      "\n",
      "Next steps:\n",
      "   1. Use the best configuration for final model training\n",
      "   2. Evaluate on test data\n",
      "   3. Consider ensembling multiple top configurations\n"
     ]
    }
   ],
   "source": [
    "# üìä Step 8: Results Analysis and Visualization\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìä Analyzing Optimization Results...\")\n",
    "\n",
    "# Helper function to convert numpy types for JSON serialization\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Check if optimization was completed successfully\n",
    "if optimization_results and 'analysis' in optimization_results:\n",
    "    analysis = optimization_results['analysis']\n",
    "    \n",
    "    # Get best configuration and results\n",
    "    best_result = analysis.get_best_trial(metric=\"val_loss\", mode=\"min\")\n",
    "    best_config = best_result.config\n",
    "    best_val_loss = best_result.last_result[\"val_loss\"]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "    print(f\"   ‚Ä¢ Validation Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ CNN Filters: {best_config['cnn_filters']}\")\n",
    "    print(f\"   ‚Ä¢ LSTM Units: {best_config['lstm_units']}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {best_config['learning_rate']:.2e}\")\n",
    "    print(f\"   ‚Ä¢ Dropout Rate: {best_config['dropout_rate']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Batch Size: {best_config['batch_size']}\")\n",
    "    \n",
    "    # Visualization of hyperparameter optimization results\n",
    "    print(f\"\\nüìà Creating optimization analysis plots...\")\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    val_losses = [trial.last_result[\"val_loss\"] for trial in analysis.trials if trial.last_result]\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('CNN-LSTM Hyperparameter Optimization Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. CNN filters vs performance\n",
    "    cnn_filters = [trial.config[\"cnn_filters\"] for trial in analysis.trials if trial.last_result]\n",
    "    axes[0, 0].scatter(cnn_filters, val_losses, alpha=0.7, color='blue', s=60)\n",
    "    axes[0, 0].set_xlabel('CNN Filters')\n",
    "    axes[0, 0].set_ylabel('Validation Loss')\n",
    "    axes[0, 0].set_title('CNN Filters vs Performance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. LSTM units vs performance\n",
    "    lstm_units = [trial.config[\"lstm_units\"] for trial in analysis.trials if trial.last_result]\n",
    "    axes[0, 1].scatter(lstm_units, val_losses, alpha=0.7, color='green', s=60)\n",
    "    axes[0, 1].set_xlabel('LSTM Units')\n",
    "    axes[0, 1].set_ylabel('Validation Loss')\n",
    "    axes[0, 1].set_title('LSTM Units vs Performance')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning rate vs performance\n",
    "    learning_rates = [trial.config[\"learning_rate\"] for trial in analysis.trials if trial.last_result]\n",
    "    axes[1, 0].scatter(learning_rates, val_losses, alpha=0.7, color='orange', s=60)\n",
    "    axes[1, 0].set_xscale('log')\n",
    "    axes[1, 0].set_xlabel('Learning Rate (log scale)')\n",
    "    axes[1, 0].set_ylabel('Validation Loss')\n",
    "    axes[1, 0].set_title('Learning Rate vs Performance')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Optimization progress\n",
    "    trial_numbers = list(range(1, len(val_losses) + 1))\n",
    "    axes[1, 1].plot(trial_numbers, val_losses, 'o-', color='purple', markersize=6, linewidth=2)\n",
    "    axes[1, 1].axhline(best_val_loss, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Best: {best_val_loss:.6f}')\n",
    "    axes[1, 1].set_xlabel('Trial Number')\n",
    "    axes[1, 1].set_ylabel('Validation Loss')\n",
    "    axes[1, 1].set_title('Optimization Progress')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance statistics\n",
    "    print(f\"\\nüìà Performance Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Mean validation loss: {np.mean(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Std validation loss: {np.std(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Min validation loss: {np.min(val_losses):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Max validation loss: {np.max(val_losses):.6f}\")\n",
    "    \n",
    "    # Save results to disk\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = \"optimization_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    best_config_serializable = convert_numpy_types(best_config)\n",
    "    \n",
    "    # Save best configuration\n",
    "    best_config_file = f\"{results_dir}/best_cnn_lstm_config_{timestamp}.json\"\n",
    "    with open(best_config_file, 'w') as f:\n",
    "        json.dump(best_config_serializable, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results Saved:\")\n",
    "    print(f\"   ‚Ä¢ Best configuration: {best_config_file}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_summary = {\n",
    "        \"optimization_method\": \"Ray Tune\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"best_config\": best_config_serializable,\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"total_trials\": len(analysis.trials),\n",
    "        \"successful_trials\": len([t for t in analysis.trials if t.status == 'TERMINATED']),\n",
    "        \"performance_stats\": {\n",
    "            \"mean_val_loss\": float(np.mean(val_losses)),\n",
    "            \"std_val_loss\": float(np.std(val_losses)),\n",
    "            \"min_val_loss\": float(np.min(val_losses)),\n",
    "            \"max_val_loss\": float(np.max(val_losses))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_file = f\"{results_dir}/cnn_lstm_hparam_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Detailed results: {results_file}\")\n",
    "    \n",
    "    # Save trial data as CSV for further analysis\n",
    "    trial_data = []\n",
    "    for i, trial in enumerate(analysis.trials):\n",
    "        if trial.last_result:\n",
    "            config_dict = convert_numpy_types(trial.config)\n",
    "            if not isinstance(config_dict, dict):\n",
    "                config_dict = {}\n",
    "            trial_info = {\n",
    "                \"trial_id\": i + 1,\n",
    "                \"val_loss\": trial.last_result[\"val_loss\"],\n",
    "                **config_dict\n",
    "            }\n",
    "            trial_data.append(trial_info)\n",
    "    \n",
    "    df_results = pd.DataFrame(trial_data)\n",
    "    csv_file = f\"{results_dir}/cnn_lstm_trials_{timestamp}.csv\"\n",
    "    df_results.to_csv(csv_file, index=False)\n",
    "    print(f\"   ‚Ä¢ Trial data (CSV): {csv_file}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Hyperparameter optimization analysis completed successfully!\")\n",
    "    print(f\"üèÜ Best configuration achieved validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    analysis_completed = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping results analysis - optimization not completed\")\n",
    "    print(\"   Run the Ray Tune optimization cell first to generate results.\")\n",
    "    analysis_completed = False\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"   1. Use the best configuration for final model training\")\n",
    "print(f\"   2. Evaluate on test data\")\n",
    "print(f\"   3. Consider ensembling multiple top configurations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
