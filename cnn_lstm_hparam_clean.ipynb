{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48577999",
   "metadata": {},
   "source": [
    "# CNN-LSTM Hyperparameter Optimization\n",
    "**Phase 2.5 - Trading RL Agent Development**\n",
    "\n",
    "This notebook implements distributed hyperparameter optimization for the CNN-LSTM model used in financial time series prediction.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Test CNN-LSTM model architecture with sample trading data\n",
    "- Implement comprehensive hyperparameter search space  \n",
    "- Execute distributed optimization using Ray Tune\n",
    "- Analyze results and identify optimal configurations\n",
    "\n",
    "## üìã Progress Tracker\n",
    "- [ ] **Step 1**: Environment Setup & Data Loading\n",
    "- [ ] **Step 2**: Model Architecture Validation\n",
    "- [ ] **Step 3**: Training Pipeline Implementation\n",
    "- [ ] **Step 4**: Hyperparameter Search Configuration\n",
    "- [ ] **Step 5**: Ray Tune Integration & Execution\n",
    "- [ ] **Step 6**: Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c006f",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Environment Setup & Data Loading\n",
    "\n",
    "Setting up the Python environment and loading sample trading data for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1353c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"üîß Environment Configuration:\")\n",
    "print(f\"   ‚Ä¢ Project root: {project_root}\")\n",
    "print(f\"   ‚Ä¢ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"   ‚Ä¢ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from src.models.cnn_lstm import CNNLSTMModel, CNNLSTMConfig, create_model\n",
    "    from src.data_pipeline import PipelineConfig, load_data, generate_features, split_by_date\n",
    "    print(\"‚úÖ Project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Ensure you're running from the project root directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35142e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preparation\n",
    "print(\"üìä Loading Sample Trading Data...\")\n",
    "\n",
    "# Load available sample data\n",
    "data_files = [f for f in os.listdir('data/') if f.startswith('sample_training_data') and f.endswith('.csv')]\n",
    "print(f\"   ‚Ä¢ Available data files: {len(data_files)}\")\n",
    "\n",
    "if data_files:\n",
    "    # Use the simple sample data for testing\n",
    "    sample_file = 'data/sample_training_data_simple_20250607_192034.csv'\n",
    "    \n",
    "    try:\n",
    "        df_sample = pd.read_csv(sample_file)\n",
    "        print(f\"‚úÖ Data loaded successfully: {sample_file}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {df_sample.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(df_sample.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Date range: {df_sample['timestamp'].iloc[0]} to {df_sample['timestamp'].iloc[-1]}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nüìã Sample Data Preview:\")\n",
    "        print(df_sample.head(3))\n",
    "        \n",
    "        # Prepare feature data (OHLCV)\n",
    "        feature_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        X_raw = df_sample[feature_columns].values.astype(np.float32)\n",
    "        \n",
    "        print(f\"\\nüî¢ Feature Data:\")\n",
    "        print(f\"   ‚Ä¢ Features: {feature_columns}\")\n",
    "        print(f\"   ‚Ä¢ Shape: {X_raw.shape}\")\n",
    "        print(f\"   ‚Ä¢ Data type: {X_raw.dtype}\")\n",
    "        \n",
    "        # Check for any data quality issues\n",
    "        print(f\"   ‚Ä¢ NaN values: {np.isnan(X_raw).sum()}\")\n",
    "        print(f\"   ‚Ä¢ Infinite values: {np.isinf(X_raw).sum()}\")\n",
    "        \n",
    "        if np.isnan(X_raw).sum() > 0 or np.isinf(X_raw).sum() > 0:\n",
    "            print(\"‚ö†Ô∏è  Data quality issues detected - may need cleaning\")\n",
    "        else:\n",
    "            print(\"‚úÖ Data quality checks passed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        df_sample = None\n",
    "        X_raw = None\n",
    "else:\n",
    "    print(\"‚ùå No sample data files found\")\n",
    "    print(\"   Run generate_sample_data.py to create sample data\")\n",
    "    df_sample = None\n",
    "    X_raw = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f779f2",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Model Architecture Validation\n",
    "\n",
    "Testing the CNN-LSTM model architecture with our sample data to ensure compatibility before hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Validation\n",
    "if X_raw is not None:\n",
    "    print(\"üèóÔ∏è Creating and Testing CNN-LSTM Model...\")\n",
    "    \n",
    "    # Prepare sequence data for time series prediction\n",
    "    sequence_length = 30  # Use 30 time steps for this test\n",
    "    step_size = 1\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Sequence length: {sequence_length}\")\n",
    "    print(f\"   ‚Ä¢ Feature dimensions: {X_raw.shape[1]}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(len(X_raw) - sequence_length):\n",
    "        X_sequences.append(X_raw[i:i + sequence_length])\n",
    "        # Predict next close price (index 3 in OHLCV)\n",
    "        y_sequences.append(X_raw[i + sequence_length, 3])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Sequence data shape: X={X_sequences.shape}, y={y_sequences.shape}\")\n",
    "    \n",
    "    # Create test model configuration\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],  # Number of features (5 for OHLCV)\n",
    "        output_size=1,                    # Single prediction output\n",
    "        cnn_filters=[32, 64],            # CNN layer sizes\n",
    "        cnn_kernel_sizes=[3, 3],         # Kernel sizes for each CNN layer\n",
    "        lstm_units=50,                   # LSTM hidden units\n",
    "        dropout=0.2,                     # Dropout rate\n",
    "        use_attention=False              # No attention for simplicity\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîß Model Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Input dimensions: {test_config.input_dim}\")\n",
    "    print(f\"   ‚Ä¢ CNN filters: {test_config.cnn_filters}\")\n",
    "    print(f\"   ‚Ä¢ CNN kernels: {test_config.cnn_kernel_sizes}\")\n",
    "    print(f\"   ‚Ä¢ LSTM units: {test_config.lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Dropout: {test_config.dropout}\")\n",
    "    \n",
    "    # Create and test model\n",
    "    try:\n",
    "        model = create_model(test_config)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model created successfully on {device}\")\n",
    "        print(f\"   ‚Ä¢ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        test_batch_size = 8\n",
    "        X_test = torch.FloatTensor(X_sequences[:test_batch_size]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = model(X_test)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Forward pass successful\")\n",
    "        print(f\"   ‚Ä¢ Input shape: {X_test.shape}\")\n",
    "        print(f\"   ‚Ä¢ Output shape: {test_output.shape}\")\n",
    "        print(f\"   ‚Ä¢ Sample predictions: {test_output.flatten()[:3].cpu().numpy()}\")\n",
    "        \n",
    "        architecture_validated = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model architecture error: {e}\")\n",
    "        architecture_validated = False\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model validation - no data available\")\n",
    "    architecture_validated = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d5599",
   "metadata": {},
   "source": [
    "## üöÑ Step 3: Training Pipeline Implementation\n",
    "\n",
    "Implementing the training pipeline that will be used for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline Implementation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_cnn_lstm_model(config, X_data, y_data, num_epochs=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for CNN-LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        config: CNNLSTMConfig object with model hyperparameters\n",
    "        X_data: Input sequences (samples, sequence_length, features)\n",
    "        y_data: Target values (samples,)\n",
    "        num_epochs: Number of training epochs\n",
    "        verbose: Print training progress\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results including losses and model\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üöÑ Training CNN-LSTM Model...\")\n",
    "        print(f\"   ‚Ä¢ Data shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Epochs: {num_epochs}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Normalize features\n",
    "    X_flat = X_data.reshape(-1, X_data.shape[-1])\n",
    "    X_normalized = scaler_X.fit_transform(X_flat).reshape(X_data.shape)\n",
    "    y_normalized = scaler_y.fit_transform(y_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_normalized, y_normalized, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Convert to tensors\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        batch_size = 32\n",
    "        \n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i:i + batch_size]\n",
    "            batch_y = y_train_tensor[i:i + batch_size]\n",
    "            \n",
    "            if len(batch_X) < 2:  # Skip very small batches\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / max(num_batches, 1)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if verbose and (epoch % 5 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"   Epoch {epoch+1:2d}/{num_epochs} - Train: {avg_train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'scalers': (scaler_X, scaler_y)\n",
    "    }\n",
    "\n",
    "# Test the training pipeline\n",
    "if architecture_validated and X_sequences is not None:\n",
    "    print(\"üß™ Testing Training Pipeline...\")\n",
    "    \n",
    "    # Test with a quick training run\n",
    "    test_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=[16, 32],\n",
    "        cnn_kernel_sizes=[3, 3],\n",
    "        lstm_units=32,\n",
    "        dropout=0.1,\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = train_cnn_lstm_model(\n",
    "            config=test_config,\n",
    "            X_data=X_sequences,\n",
    "            y_data=y_sequences,\n",
    "            num_epochs=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training pipeline test successful!\")\n",
    "        print(f\"   ‚Ä¢ Final train loss: {results['final_train_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Final val loss: {results['final_val_loss']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Best val loss: {results['best_val_loss']:.6f}\")\n",
    "        \n",
    "        training_pipeline_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training pipeline error: {e}\")\n",
    "        training_pipeline_ready = False\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping training pipeline test - architecture not validated\")\n",
    "    training_pipeline_ready = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd351f",
   "metadata": {},
   "source": [
    "## üîç Step 4: Hyperparameter Search Configuration\n",
    "\n",
    "Defining the search space and strategies for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5029af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Configuration\n",
    "print(\"üîç Defining Hyperparameter Search Space...\")\n",
    "\n",
    "# Define comprehensive search space\n",
    "def get_hyperparameter_search_space():\n",
    "    \"\"\"Define the hyperparameter search space for CNN-LSTM optimization.\"\"\"\n",
    "    return {\n",
    "        # Architecture parameters\n",
    "        \"cnn_filters\": [\n",
    "            [16, 32],\n",
    "            [32, 64], \n",
    "            [64, 128],\n",
    "            [16, 32, 64]\n",
    "        ],\n",
    "        \"cnn_kernel_sizes\": [\n",
    "            [3, 3],\n",
    "            [3, 5],\n",
    "            [5, 5],\n",
    "            [3, 3, 3]  # For 3-layer CNN\n",
    "        ],\n",
    "        \"lstm_units\": [32, 50, 64, 100, 128],\n",
    "        \"dropout\": [0.1, 0.2, 0.3, 0.4],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"learning_rate\": [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "        \"batch_size\": [16, 32, 64],\n",
    "        \"num_epochs\": [10, 15, 20]\n",
    "    }\n",
    "\n",
    "# Get search space\n",
    "search_space = get_hyperparameter_search_space()\n",
    "\n",
    "print(\"üìä Hyperparameter Search Space:\")\n",
    "for param, values in search_space.items():\n",
    "    print(f\"   ‚Ä¢ {param:15} : {len(values)} options\")\n",
    "    if len(values) <= 5:\n",
    "        print(f\"     {values}\")\n",
    "    else:\n",
    "        print(f\"     {values[:3]}...{values[-2:]}\")\n",
    "\n",
    "# Calculate search space size\n",
    "total_combinations = 1\n",
    "for param, values in search_space.items():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"\\nüìà Search Space Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total parameters: {len(search_space)}\")\n",
    "print(f\"   ‚Ä¢ Total combinations: {total_combinations:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated time (5min/trial): {total_combinations * 5 / 60:.1f} hours\")\n",
    "\n",
    "# Define optimization strategy\n",
    "print(f\"\\n‚ö° Optimization Strategy:\")\n",
    "print(f\"   ‚Ä¢ Algorithm: Random Search + Early Stopping\")\n",
    "print(f\"   ‚Ä¢ Trials: 20-50 (subset of full space)\")\n",
    "print(f\"   ‚Ä¢ Early stopping: ASHA scheduler\")\n",
    "print(f\"   ‚Ä¢ Metric: Validation loss\")\n",
    "print(f\"   ‚Ä¢ Mode: Minimize\")\n",
    "\n",
    "# Create Ray Tune compatible search space\n",
    "def get_ray_tune_search_space():\n",
    "    \"\"\"Convert search space to Ray Tune format.\"\"\"\n",
    "    try:\n",
    "        from ray import tune\n",
    "        return {\n",
    "            \"cnn_filters\": tune.choice([\n",
    "                [16, 32],\n",
    "                [32, 64], \n",
    "                [64, 128]\n",
    "            ]),\n",
    "            \"cnn_kernel_sizes\": tune.choice([\n",
    "                [3, 3],\n",
    "                [3, 5],\n",
    "                [5, 5]\n",
    "            ]),\n",
    "            \"lstm_units\": tune.choice([32, 50, 64, 100]),\n",
    "            \"dropout\": tune.uniform(0.1, 0.4),\n",
    "            \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "            \"batch_size\": tune.choice([16, 32, 64]),\n",
    "            \"num_epochs\": tune.choice([10, 15, 20])\n",
    "        }\n",
    "    except ImportError:\n",
    "        print(\"   Ray Tune not available - will use manual search\")\n",
    "        return None\n",
    "\n",
    "ray_search_space = get_ray_tune_search_space()\n",
    "\n",
    "if ray_search_space:\n",
    "    print(f\"‚úÖ Ray Tune search space created\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Ray Tune search space not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e234977",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Ray Tune Integration & Execution\n",
    "\n",
    "Setting up distributed hyperparameter optimization with Ray Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd08a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tune Integration & Setup\n",
    "print(\"‚ö° Setting up Ray Tune for Distributed Optimization...\")\n",
    "\n",
    "# Ray initialization with robust error handling\n",
    "def initialize_ray():\n",
    "    \"\"\"Initialize Ray with fallback strategies.\"\"\"\n",
    "    try:\n",
    "        import ray\n",
    "        \n",
    "        # Check if Ray is already running\n",
    "        if ray.is_initialized():\n",
    "            print(\"   ‚Ä¢ Ray already initialized\")\n",
    "            return True\n",
    "            \n",
    "        # Try to start Ray\n",
    "        ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "        print(f\"   ‚Ä¢ Ray initialized successfully\")\n",
    "        print(f\"   ‚Ä¢ Available resources: {ray.available_resources()}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Ray initialization failed: {e}\")\n",
    "        return False\n",
    "\n",
    "ray_available = initialize_ray()\n",
    "\n",
    "# Define Ray Tune training function\n",
    "def ray_tune_train_function(config):\n",
    "    \"\"\"Training function compatible with Ray Tune.\"\"\"\n",
    "    from ray import train\n",
    "    \n",
    "    # Create model config from Ray Tune hyperparameters\n",
    "    model_config = CNNLSTMConfig(\n",
    "        input_dim=X_sequences.shape[-1],\n",
    "        output_size=1,\n",
    "        cnn_filters=config[\"cnn_filters\"],\n",
    "        cnn_kernel_sizes=config[\"cnn_kernel_sizes\"],\n",
    "        lstm_units=config[\"lstm_units\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        use_attention=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    results = train_cnn_lstm_model(\n",
    "        config=model_config,\n",
    "        X_data=X_sequences,\n",
    "        y_data=y_sequences,\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        verbose=False  # Reduce output for Ray Tune\n",
    "    )\n",
    "    \n",
    "    # Report metrics to Ray Tune\n",
    "    train.report({\n",
    "        \"train_loss\": results[\"final_train_loss\"],\n",
    "        \"val_loss\": results[\"final_val_loss\"], \n",
    "        \"best_val_loss\": results[\"best_val_loss\"]\n",
    "    })\n",
    "\n",
    "# Configure Ray Tune experiment\n",
    "def setup_ray_tune_experiment(num_samples=12, max_concurrent_trials=3):\n",
    "    \"\"\"Setup Ray Tune experiment configuration.\"\"\"\n",
    "    if not ray_available:\n",
    "        print(\"   ‚ùå Ray not available - cannot setup experiment\")\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        from ray import tune\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "        \n",
    "        # Early stopping scheduler\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            max_t=20,  # Maximum epochs\n",
    "            grace_period=5,  # Minimum epochs before stopping\n",
    "            reduction_factor=2\n",
    "        )\n",
    "        \n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"./ray_results/cnn_lstm_hparam_{timestamp}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"   ‚úÖ Ray Tune experiment configured:\")\n",
    "        print(f\"      ‚Ä¢ Scheduler: ASHA (early stopping)\")\n",
    "        print(f\"      ‚Ä¢ Number of trials: {num_samples}\")\n",
    "        print(f\"      ‚Ä¢ Max concurrent: {max_concurrent_trials}\")\n",
    "        print(f\"      ‚Ä¢ Output directory: {output_dir}\")\n",
    "        \n",
    "        return scheduler, output_dir, True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚ùå Ray Tune not available: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "# Setup experiment\n",
    "if ray_available and training_pipeline_ready:\n",
    "    scheduler, output_dir, tune_ready = setup_ray_tune_experiment()\n",
    "    print(f\"   Ray Tune setup: {'‚úÖ Ready' if tune_ready else '‚ùå Failed'}\")\n",
    "else:\n",
    "    tune_ready = False\n",
    "    print(\"   ‚ö†Ô∏è  Ray Tune setup skipped - prerequisites not met\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f358c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Hyperparameter Optimization\n",
    "print(\"üöÄ Executing CNN-LSTM Hyperparameter Optimization...\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "optimization_results = None\n",
    "best_config = None\n",
    "\n",
    "if tune_ready and ray_search_space:\n",
    "    print(\"\\n‚ö° Running Ray Tune Optimization...\")\n",
    "    \n",
    "    try:\n",
    "        from ray import tune\n",
    "        \n",
    "        # Run hyperparameter optimization\n",
    "        analysis = tune.run(\n",
    "            ray_tune_train_function,\n",
    "            config=ray_search_space,\n",
    "            scheduler=scheduler,\n",
    "            num_samples=12,  # Number of trials\n",
    "            resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    "            storage_path=os.path.abspath(output_dir),\n",
    "            name=\"cnn_lstm_optimization\",\n",
    "            verbose=1,\n",
    "            raise_on_failed_trial=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Ray Tune optimization completed!\")\n",
    "        \n",
    "        # Extract results\n",
    "        results_df = analysis.results_df\n",
    "        \n",
    "        if len(results_df) > 0 and 'val_loss' in results_df.columns:\n",
    "            # Get best trial\n",
    "            successful_trials = results_df[results_df['val_loss'].notna()]\n",
    "            \n",
    "            if len(successful_trials) > 0:\n",
    "                best_idx = successful_trials['val_loss'].idxmin()\n",
    "                best_trial = successful_trials.loc[best_idx]\n",
    "                \n",
    "                best_config = {\n",
    "                    'cnn_filters': best_trial.get('config/cnn_filters'),\n",
    "                    'cnn_kernel_sizes': best_trial.get('config/cnn_kernel_sizes'),\n",
    "                    'lstm_units': best_trial.get('config/lstm_units'),\n",
    "                    'dropout': best_trial.get('config/dropout'),\n",
    "                    'learning_rate': best_trial.get('config/learning_rate'),\n",
    "                    'batch_size': best_trial.get('config/batch_size'),\n",
    "                    'num_epochs': best_trial.get('config/num_epochs')\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "                print(f\"   ‚Ä¢ Validation Loss: {best_trial['val_loss']:.6f}\")\n",
    "                for key, value in best_config.items():\n",
    "                    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "                \n",
    "                optimization_results = {\n",
    "                    'method': 'Ray Tune',\n",
    "                    'results_df': results_df,\n",
    "                    'best_config': best_config,\n",
    "                    'best_val_loss': best_trial['val_loss'],\n",
    "                    'total_trials': len(results_df),\n",
    "                    'successful_trials': len(successful_trials)\n",
    "                }\n",
    "            else:\n",
    "                print(\"‚ùå No successful trials found\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid results returned from Ray Tune\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ray Tune execution failed: {e}\")\n",
    "        tune_ready = False\n",
    "\n",
    "# Fallback: Manual grid search\n",
    "if not tune_ready or optimization_results is None:\n",
    "    print(\"\\nüîß Running Manual Grid Search (Fallback)...\")\n",
    "    \n",
    "    # Define a smaller search space for manual testing\n",
    "    manual_search_configs = [\n",
    "        {\n",
    "            'cnn_filters': [32, 64],\n",
    "            'cnn_kernel_sizes': [3, 3],\n",
    "            'lstm_units': 50,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 10\n",
    "        },\n",
    "        {\n",
    "            'cnn_filters': [16, 32],\n",
    "            'cnn_kernel_sizes': [3, 3], \n",
    "            'lstm_units': 32,\n",
    "            'dropout': 0.1,\n",
    "            'learning_rate': 0.005,\n",
    "            'batch_size': 16,\n",
    "            'num_epochs': 10\n",
    "        },\n",
    "        {\n",
    "            'cnn_filters': [64, 128],\n",
    "            'cnn_kernel_sizes': [3, 5],\n",
    "            'lstm_units': 100,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 0.0005,\n",
    "            'batch_size': 64,\n",
    "            'num_epochs': 15\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    manual_results = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for i, config in enumerate(manual_search_configs):\n",
    "        print(f\"\\n   Trial {i+1}/{len(manual_search_configs)}: {config['cnn_filters']}, {config['lstm_units']} units\")\n",
    "        \n",
    "        try:\n",
    "            # Create model config\n",
    "            model_config = CNNLSTMConfig(\n",
    "                input_dim=X_sequences.shape[-1],\n",
    "                output_size=1,\n",
    "                cnn_filters=config['cnn_filters'],\n",
    "                cnn_kernel_sizes=config['cnn_kernel_sizes'],\n",
    "                lstm_units=config['lstm_units'],\n",
    "                dropout=config['dropout'],\n",
    "                use_attention=False\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            results = train_cnn_lstm_model(\n",
    "                config=model_config,\n",
    "                X_data=X_sequences,\n",
    "                y_data=y_sequences,\n",
    "                num_epochs=config['num_epochs'],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            trial_result = config.copy()\n",
    "            trial_result.update({\n",
    "                'val_loss': results['final_val_loss'],\n",
    "                'train_loss': results['final_train_loss'],\n",
    "                'best_val_loss': results['best_val_loss']\n",
    "            })\n",
    "            manual_results.append(trial_result)\n",
    "            \n",
    "            if results['final_val_loss'] < best_val_loss:\n",
    "                best_val_loss = results['final_val_loss']\n",
    "                best_config = config.copy()\n",
    "            \n",
    "            print(f\"      ‚úÖ Val Loss: {results['final_val_loss']:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Trial failed: {e}\")\n",
    "    \n",
    "    if manual_results:\n",
    "        optimization_results = {\n",
    "            'method': 'Manual Grid Search',\n",
    "            'results': manual_results,\n",
    "            'best_config': best_config,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'total_trials': len(manual_search_configs),\n",
    "            'successful_trials': len(manual_results)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüèÜ Manual Search Best Configuration:\")\n",
    "        print(f\"   ‚Ä¢ Validation Loss: {best_val_loss:.6f}\")\n",
    "        for key, value in best_config.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1eb7e7",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Analysis & Visualization\n",
    "\n",
    "Analyzing optimization results and visualizing performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3640e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 8: Results Analysis and Visualization\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìä Analyzing Optimization Results...\")\n",
    "\n",
    "# Helper function to convert numpy types to native Python types\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Recursively convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # numpy scalars\n",
    "        return obj.item()\n",
    "    elif hasattr(obj, 'tolist'):  # numpy arrays\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Get best configuration and results\n",
    "best_result = analysis.get_best_trial(metric=\"val_loss\", mode=\"min\")\n",
    "best_config = best_result.config\n",
    "best_val_loss = best_result.last_result[\"val_loss\"]\n",
    "\n",
    "print(f\"\\nüéØ Optimization Summary:\")\n",
    "print(f\"   ‚Ä¢ Method: Ray Tune\")\n",
    "print(f\"   ‚Ä¢ Total trials: {len(analysis.trials)}\")\n",
    "print(f\"   ‚Ä¢ Successful trials: {len([t for t in analysis.trials if t.status == 'TERMINATED'])}\")\n",
    "print(f\"   ‚Ä¢ Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('CNN-LSTM Hyperparameter Optimization Results\\nRay Tune', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Validation loss distribution\n",
    "val_losses = [trial.last_result[\"val_loss\"] for trial in analysis.trials if trial.last_result]\n",
    "axes[0, 0].hist(val_losses, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(best_val_loss, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Best: {best_val_loss:.6f}')\n",
    "axes[0, 0].set_xlabel('Validation Loss')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Validation Losses')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. LSTM units vs performance\n",
    "lstm_units = [trial.config[\"lstm_units\"] for trial in analysis.trials if trial.last_result]\n",
    "axes[0, 1].scatter(lstm_units, val_losses, alpha=0.7, color='green', s=60)\n",
    "axes[0, 1].set_xlabel('LSTM Units')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('LSTM Units vs Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning rate vs performance\n",
    "learning_rates = [trial.config[\"learning_rate\"] for trial in analysis.trials if trial.last_result]\n",
    "axes[1, 0].scatter(learning_rates, val_losses, alpha=0.7, color='orange', s=60)\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].set_xlabel('Learning Rate (log scale)')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].set_title('Learning Rate vs Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Optimization progress\n",
    "trial_numbers = list(range(1, len(val_losses) + 1))\n",
    "axes[1, 1].plot(trial_numbers, val_losses, 'o-', color='purple', markersize=6, linewidth=2)\n",
    "axes[1, 1].axhline(best_val_loss, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Best: {best_val_loss:.6f}')\n",
    "axes[1, 1].set_xlabel('Trial Number')\n",
    "axes[1, 1].set_ylabel('Validation Loss')\n",
    "axes[1, 1].set_title('Optimization Progress')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance statistics\n",
    "print(f\"\\nüìà Performance Analysis:\")\n",
    "print(f\"   ‚Ä¢ Mean validation loss: {np.mean(val_losses):.6f}\")\n",
    "print(f\"   ‚Ä¢ Std validation loss: {np.std(val_losses):.6f}\")\n",
    "print(f\"   ‚Ä¢ Min validation loss: {np.min(val_losses):.6f}\")\n",
    "print(f\"   ‚Ä¢ Max validation loss: {np.max(val_losses):.6f}\")\n",
    "\n",
    "# Save results to disk\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = \"optimization_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "best_config_serializable = convert_numpy_types(best_config)\n",
    "\n",
    "# Save best configuration\n",
    "best_config_file = f\"{results_dir}/best_cnn_lstm_config_{timestamp}.json\"\n",
    "with open(best_config_file, 'w') as f:\n",
    "    json.dump(best_config_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results Saved:\")\n",
    "print(f\"   ‚Ä¢ Best configuration: {best_config_file}\")\n",
    "\n",
    "# Save detailed results\n",
    "results_summary = {\n",
    "    \"optimization_method\": \"Ray Tune\",\n",
    "    \"timestamp\": timestamp,\n",
    "    \"best_config\": best_config_serializable,\n",
    "    \"best_val_loss\": float(best_val_loss),\n",
    "    \"total_trials\": len(analysis.trials),\n",
    "    \"successful_trials\": len([t for t in analysis.trials if t.status == 'TERMINATED']),\n",
    "    \"performance_stats\": {\n",
    "        \"mean_val_loss\": float(np.mean(val_losses)),\n",
    "        \"std_val_loss\": float(np.std(val_losses)),\n",
    "        \"min_val_loss\": float(np.min(val_losses)),\n",
    "        \"max_val_loss\": float(np.max(val_losses))\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = f\"{results_dir}/cnn_lstm_hparam_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"   ‚Ä¢ Detailed results: {results_file}\")\n",
    "\n",
    "# Save trial data as CSV for further analysis\n",
    "trial_data = []\n",
    "for i, trial in enumerate(analysis.trials):\n",
    "    if trial.last_result:\n",
    "        config_dict = convert_numpy_types(trial.config)\n",
    "        if not isinstance(config_dict, dict):\n",
    "            config_dict = {}\n",
    "        trial_info = {\n",
    "            \"trial_id\": i + 1,\n",
    "            \"val_loss\": trial.last_result[\"val_loss\"],\n",
    "            **config_dict\n",
    "        }\n",
    "        trial_data.append(trial_info)\n",
    "\n",
    "df_results = pd.DataFrame(trial_data)\n",
    "csv_file = f\"{results_dir}/cnn_lstm_trials_{timestamp}.csv\"\n",
    "df_results.to_csv(csv_file, index=False)\n",
    "print(f\"   ‚Ä¢ Trial data (CSV): {csv_file}\")\n",
    "\n",
    "print(f\"\\nüéâ Hyperparameter optimization completed successfully!\")\n",
    "print(f\"üèÜ Best configuration achieved validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"   1. Use the best configuration for final model training\")\n",
    "print(f\"   2. Evaluate on test data\")\n",
    "print(f\"   3. Consider ensembling multiple top configurations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
