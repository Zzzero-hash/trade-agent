# PPO algorithm hyperparameters

ppo:
  algorithm: PPO
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.05
  ent_coef_final: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  seed: ${seed}
  reward_scaling:
    scale: 1.0
    clip_range: 10.0
