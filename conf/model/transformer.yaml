model_type: transformer
model_config:
  input_size: 17
  d_model: 256
  nhead: 8
  num_layers: 6
  dim_feedforward: 1024
  output_size: 1
  dropout: 0.2
  attention_dropout: 0.2
  sequence_length: 60
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.00001
  random_state: ${random_state}
cv_config:
  n_splits: 3
  gap: 10

tuning_config:
  enable_tuning: false
  scoring_metric: neg_mean_squared_error
  param_grid:
    d_model: [32, 64]
    nhead: [4, 8]
    num_layers: [1, 2]
    learning_rate: [0.001, 0.01]
    dropout: [0.1, 0.2]
  n_trials: 8
