# TD3 Agent Configuration for Trading RL
# Twin Delayed Deep Deterministic Policy Gradient hyperparameters

# Learning parameters
learning_rate: 3e-4
gamma: 0.99              # Discount factor
tau: 0.005               # Soft update rate for target networks

# Training parameters
batch_size: 256          # Replay buffer batch size
buffer_capacity: 1000000 # Replay buffer capacity

# Network architecture
hidden_dims: [256, 256]  # Hidden layer dimensions

# TD3 specific parameters
policy_delay: 2          # Update policy every N critic updates
target_noise: 0.2        # Noise added to target actions (policy smoothing)
noise_clip: 0.5          # Clipping range for target noise
exploration_noise: 0.1   # Exploration noise during training

# Trading-specific parameters
position_scaling: 1.0    # Scale factor for position sizes
risk_penalty: 0.01      # Penalty for large position changes

# Training schedule
warmup_steps: 1000      # Steps before training begins
eval_frequency: 5000    # Evaluate agent every N steps
save_frequency: 10000   # Save checkpoint every N steps

# Environment integration
action_space: "continuous"    # Action space type
action_bounds: [-1.0, 1.0]   # Position size bounds
state_preprocessing: "normalize"  # State preprocessing method

# Advanced features
gradient_clipping: 1.0       # Gradient clipping value
layer_normalization: false   # Use layer normalization in networks
orthogonal_init: false      # Use orthogonal weight initialization
