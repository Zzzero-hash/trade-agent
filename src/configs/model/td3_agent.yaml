# SAC Agent Configuration for Trading RL 
# Soft Actor Critic hyperparameters (Ray RLlib compatible)

# IMPORTANT: TD3 has been removed from Ray RLlib 2.38.0+
# This configuration uses SAC for Ray RLlib integration
# Custom TD3 implementation remains available for local testing

# Learning parameters
actor_lr: 3e-5               # Actor learning rate (lower for stability)
critic_lr: 3e-4              # Critic learning rate  
alpha_lr: 3e-4               # Entropy coefficient learning rate (SAC-specific)
gamma: 0.99                  # Discount factor
tau: 0.005                   # Soft update rate for target networks

# Training parameters
batch_size: 256              # Replay buffer batch size
buffer_capacity: 1000000     # Replay buffer capacity

# Network architecture
hidden_dims: [256, 256]      # Hidden layer dimensions

# SAC specific parameters
twin_q: true                 # Use twin Q-networks (similar to TD3's twin critics)
initial_alpha: 0.2           # Initial entropy coefficient
target_entropy: "auto"      # Target entropy (auto = -action_dim)
n_step: 1                    # N-step returns

# Trading-specific parameters
position_scaling: 1.0        # Scale factor for position sizes
risk_penalty: 0.01          # Penalty for large position changes

# Training schedule
warmup_steps: 1000          # Steps before training begins
eval_frequency: 5000        # Evaluate agent every N steps
save_frequency: 10000       # Save checkpoint every N steps

# Environment integration
action_space: "continuous"      # Action space type
action_bounds: [-1.0, 1.0]     # Position size bounds
state_preprocessing: "normalize"  # State preprocessing method

# Advanced features
gradient_clipping: 1.0         # Gradient clipping value
layer_normalization: false     # Use layer normalization in networks
orthogonal_init: false        # Use orthogonal weight initialization
